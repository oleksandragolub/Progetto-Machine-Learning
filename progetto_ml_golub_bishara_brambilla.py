# -*- coding: utf-8 -*-
"""progetto_ML_Golub_Bishara_Brambilla.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SC-vOJSLl2yx41B_to3R3O2T7DIDjVGN

## **Machine Learning**

---
---

Università degli Studi Milano Bicocca \
CdLM Informatica — A.A 2024/2025

---
---

#### **Componenti del gruppo:**
— Oleksandra Golub (856706) \
— Giovanni Bishara (869532) \
— Federico Brambilla (886046)

---
---

### **Librerie e strumenti per il preprocessing e la modellazione in ML**

Queste righe di codice importano le librerie necessarie per il preprocessing dei dati, la gestione dei dataset e la loro visualizzazione, oltre a strumenti per il bilanciamento dei dati e l'implementazione di modelli di machine learning:
- **kagglehub** è una libreria per scaricare dataset direttamente da Kaggle (permette di scaricare automaticamente i dati senza doverli caricare manualmente).
- il modulo **os** permette di interagire con il sistema operativo (utile per verificare la posizione di un dataset scaricato).
- **pandas** è una libreria per la gestione e manipolazione dei dataset (permette di leggere, analizzare e trasformare i dati in formato tabellare).
- **matplotlib.pyplot** è una libreria per creare grafici e visualizzazioni (aiuta a visualizzare distribuzioni, istogrammi e grafici di correlazione).
- **seaborn** è una libreria per la visualizzazione avanzata dei dati basata su **matplotlib** (aiuta a creare grafici più leggibili e dettagliati per analizzare le relazioni tra le variabili).
- **numpy** è una libreria per il calcolo numerico e la gestione degli array (utilizzata per manipolare dati numerici, eseguire calcoli e operazioni vettorial).
- il modulo **scipy.stats** fornisce funzioni per calcoli statistici avanzati. In particolare calcola l'intervallo di confidenza per una distribuzione t di Student.
- **SMOTE** (Synthetic Minority Over-sampling Technique) è una tecnica di oversampling per bilanciare dataset sbilanciati. Genera nuovi esempi sintetici della classe minoritaria (es. se i pazienti diabetici sono meno numerosi, SMOTE genera nuovi dati simili per bilanciare il dataset).
- **StandardScaler** e **PowerTransformer** sono strumenti per trasformare e normalizzare i dati:
  1.   **StandardScaler** ridimensiona i dati in modo che abbiano *media 0* e *deviazione standard 1* (necessario per molti algoritmi di ML).
  2.   **PowerTransformer** applica una trasformazione (es. logaritmica) per rendere più normale la distribuzione dei dati.
- **enable_iterative_imputer** è necessario per abilitare **IterativeImputer**, che è ancora un'API sperimentale in **scikit-learn** (permette di eseguire un'imputazione avanzata dei valori mancanti stimandoli in base alle altre feature).
- **IterativeImputer** è un metodo per riempire i valori mancanti nei dati in modo intelligente (stima i valori basandosi sulle altre feature, anziché usare semplici medie o mediane).
- **PCA** (analisi delle componenti principali) è una tecnica di riduzione della dimensionalità che trasforma le feature di un dataset in nuove variabili (componenti principali), preservando la massima varianza possibile.
- **Axes3D** è un modulo di **Matplotlib** che permette di creare grafici tridimensionali. Viene spesso usato con PCA per rappresentare i dati in uno spazio ridotto a 3 dimensioni.
- **train_test_split** è una funzione di **scikit-learn** per suddividere i dati in training set e test set (permette di addestrare il modello su una parte dei dati e testarlo su un'altra parte per valutarne le prestazioni).

Strumenti per la gestione della cross-validation e l'ottimizzazione del modello:
- **KFold** divide il dataset in k sottoinsiemi (fold) per valutare le performance del modello in modo più robusto.
- **StratifiedKFold** è un variante di KFold che mantiene la stessa distribuzione delle classi in ogni fold, utile per dataset sbilanciati.
- **cross_val_score** esegue la cross-validation e restituisce i punteggi delle metriche scelte per ogni fold.
- **cross_validate** permette di valutare più metriche contemporaneamente durante la cross-validation.
- **GridSearchCV** esegue una ricerca esaustiva dei migliori iperparametri testando tutte le combinazioni specificate.
- **RandomizedSearchCV** simile a GridSearchCV, ma esplora casualmente solo alcune combinazioni, rendendolo più veloce.
- **uniform, randint, loguniform** da **scipy.stats** definiscono distribuzioni casuali per la ricerca degli iperparametri in **RandomizedSearchCV**.
- **make_scorer** consente di creare metriche personalizzate per la valutazione del modello.
- **Patch** viene usato per personalizzare le legende nei grafici.
- **cm (colormap)** è la libreria di **matplotlib** per gestire le mappe di colori nei grafici.

Modelli di Machine Learning:
- **DecisionTreeClassifier** implementa un albero di decisione, un modello che suddivide i dati in base a domande sequenziali per fare previsioni.
- **plot_tree** è una funzione che permette di visualizzare la struttura dell'albero di decisione addestrato. È utile per interpretare il modello, capire quali feature sono state utilizzate per le suddivisioni e come il modello prende le decisioni.
- **LogisticRegression** è un modello statistico di classificazione binaria basato sulla funzione logistica (sigmoide). Stima la probabilità che un'osservazione appartenga a una classe specifica.
- **SVC (Support Vector Classifier)** è un potente modello per la classificazione. Trova un iperpiano ottimale che separa le classi massimizzando la distanza tra loro. Può utilizzare diversi kernel (lineare, polinomiale, RBF, ecc).
- **GaussianNB (Naive Bayes Gaussiano)** è basato sul **Teorema di Bayes**, assume che le feature siano indipendenti tra loro. La variante GaussianNB è utilizzata quando le feature seguono una distribuzione normale (gaussiana). È molto veloce e adatto a dataset di grandi dimensioni.
- **RandomForestClassifier** è un modello ensemble che combina più alberi di decisione per migliorare l'accuratezza e ridurre l'overfitting. Ogni albero riceve un sottoinsieme casuale dei dati (bagging) e fa una previsione. Il risultato finale è dato dalla maggioranza delle previsioni degli alberi. È molto potente, flessibile e meno sensibile ai dati rumorosi.

Metriche di valutazione dei modelli:
- **accuracy_score(y_true, y_pred)** serve per calcolare l'accuratezza, ossia la percentuale di previsioni corrette.
- **precision_score(y_true, y_pred)** serve per indicare la percentuale di predizioni corrette tra quelle che il modello ha classificato come positive. Utile per problemi con classi sbilanciate.
- **recall_score(y_true, y_pred)** serve per misurare quanti dei veri positivi il modello è riuscito a identificare.
- **f1_score(y_true, y_pred)** serve per ottenere la media armonica tra precisione e recall. Utile quando è importante trovare un bilanciamento tra precisione e recall.
- **confusion_matrix(y_true, y_pred)** serve per creare una matrice di confusione, che mostra il numero di predizioni corrette e errate suddivise per classe. Utile per analizzare dove il modello sta sbagliando.
- **roc_curve(y_true, y_score)** misura la capacità di un modello di classificazione binaria nel distinguere tra classi positive e negative. Mostra la relazione tra il True Positive Rate (TPR) e il False Positive Rate (FPR) a diversi livelli di soglia.
- **auc(fpr, tpr)** è una metrica (Area Under Curve) che misura l'area sotto la curva ROC (Receiver Operating Characteristic). Un valore di AUC=1 indica un classificatore perfetto, mentre AUC=0.5 indica un classificatore casuale. Da notare che L'AUC è particolarmente utile quando si confrontano diversi modelli: più è alto, migliore è la capacità predittiva del modello.
"""

import kagglehub
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.stats as st

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.decomposition import PCA
from mpl_toolkits.mplot3d import Axes3D

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV
from scipy.stats import uniform, randint, loguniform

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import roc_curve, auc, make_scorer
from matplotlib.patches import Patch
from matplotlib import cm

"""---
---
### **Dataset**

#### **Caricamento del dataset**

Queste righe di codice servono per scaricare, caricare e verificare il dataset sui pazienti diabetici:
1. Scaricamento del dataset da Kaggle
  - si utilizza `kagglehub.dataset_download("lara311/diabetes-dataset-using-many-medical-metrics")` per scaricare automaticamente il dataset "`diabetes-dataset-using-many-medical-metrics`" da [Kaggle](https://www.kaggle.com/datasets/lara311/diabetes-dataset-using-many-medical-metrics/data).
  - la variabile `dataset_path` conterrà il percorso della cartella in cui il dataset è stato salvato.
  - il percorso del dataset viene stampato per verificare che l'operazione sia riuscita correttamente.

2. Definizione del percorso locale del dataset
  - `dataset_path` viene definito manualmente per specificare dove è stato scaricato il dataset.
  - normalmente, kagglehub salva i file in una cartella cache, quindi impostiamo manualmente il percorso corretto per evitarne la ricerca.
  - alcune versioni di kagglehub non restituiscono il percorso completo del dataset, quindi questa operazione aiuta a evitare problemi di accesso ai file.

3. Verifica dei file scaricati
  - `os.listdir(dataset_path)` stampa l'elenco dei file presenti nella cartella del dataset.
  - questo permette di verificare quali file sono stati effettivamente scaricati.
  - è utile soprattutto quando non si è sicuri del nome esatto del file CSV nel dataset.

4. Caricamento del dataset in Pandas
  - `pd.read_csv(dataset_path + "/diabetes (1).csv")` legge il file CSV e lo carica in un DataFrame Pandas.

Ora il dataset è pronto per essere esplorato e analizzato con Pandas!
"""

dataset_path = kagglehub.dataset_download("lara311/diabetes-dataset-using-many-medical-metrics")
print("Il dataset è stato scaricato in:", dataset_path)

dataset_path = "/root/.cache/kagglehub/datasets/lara311/diabetes-dataset-using-many-medical-metrics/versions/1"

print(os.listdir(dataset_path))

df = pd.read_csv(dataset_path + "/diabetes (1).csv")

"""* `df.head()` restituisce le prime 5 righe (di default) del DataFrame Pandas. È utile per verificare se i dati sono stati caricati correttamente e per analizzare un'anteprima del dataset."""

df.head()

"""*  `df` visualizza tutto il dataset presente nel DataFrame Pandas."""

df

"""#### **Casting delle colonne e/o target**

È essenziale verificare e, se necessario, effettuare il cast dei tipi delle feature e del target. Questo passaggio garantisce che i dati vengano interpretati correttamente durante le analisi, evitando errori dovuti a una rappresentazione errata o incoerente dei tipi di dati disponibili.

*   `df.dtypes` determina se una colonna contiene numeri interi (int64), numeri decimali (float64), stringhe (object), valori booleani (bool), o categorie (category).
*   `dtype: object` significa che l'elenco dei tipi di dato è memorizzato come un oggetto generico in Pandas.
"""

# Tipi di dato per capire se servono cast
df.dtypes

"""Da notare che la colonna `Outcome` deve essere castata a `category`, perché rappresenta una variabile categorica (classi 0 e 1).

Per il resto delle colonne non è necessario alcun cast. Le variabili sono già correttamente rappresentate come float64 per valori continui o int64 per valori discreti.
"""

# Cast della colonna Outcome a category
df['Outcome'] = df['Outcome'].astype('category')

# Verifica dei tipi aggiornati
df.dtypes

"""#### **Controllo iniziale del dataset**

Questa sezione include una serie di analisi preliminari per comprendere la struttura, la distribuzione e le relazioni tra le feature del dataset prima di qualsiasi preprocessing.

*   `df.shape` mostra il numero di righe e colonne del dataset.
"""

df.shape

"""* `df.isnull().sum()` è un comando che consente di verificare la presenza di valori mancanti (nulli) in ogni colonna di un DataFrame."""

df.isnull().sum()

"""Siccome non è necessario eliminare i valori nulli, procediamo con:

* `df.duplicated()` è un comando che permette di identificare il numero di righe duplicate all'interno di un DataFrame.
"""

df.duplicated().sum()

"""Siccome non è necessario eliminare i valori duplicati, procediamo con:
*  `df.info()` fornisce informazioni riassuntive sulla struttura del DataFrame Pandas, dove:
  - `<class 'pandas.core.frame.DataFrame'>` indica che il dataset è un DataFrame Pandas.
  - `RangeIndex: 768 entries, 0 to 767` indica che il dataset ha 768 righe, numerate da 0 a 767.
  - `Data columns (total 9 columns)`indica che il dataset contiene 9 colonne.
  - Dettagli delle colonne includono i nominativi delle colonne, Non-Null Count (si vede che tutte le colonne hanno 768 valori non nulli, quindi non ci sono valori mancanti) e Dtype (per indicare colonne che contengono numeri interi e colonne che contengono numeri decimali)
  - `dtypes: category(1), float64(2), int64(6)`,  indica che nel dataset ci sono 1 colonna con tipo categorico, 2 colonne con tipo float64 (numeri decimali) e 6 colonne con tipo int64 (numeri interi).
  - `memory usage: 54.1 KB`indica che il dataset occupa 49.0 KB di memoria.
"""

df.info()

"""#### **Descrizione del dataset**

Il dataset analizza le caratteristiche cliniche di pazienti per la classificazione binaria della presenza o assenza di diabete. Ogni riga rappresenta un paziente, mentre le colonne contengono misurazioni mediche e dati anamnestici.

##### **Feature del dataset**
Le variabili disponibili nel dataset includono misurazioni fisiologiche, anamnestiche e indici metabolici, espressi come valori numerici continui o interi:

- Pregnancies: numero di gravidanze della paziente (solo per donne, nei dati non è specificato il sesso).
- Glucose (mg/dL): livello di glucosio nel sangue a digiuno.
- BloodPressure (mmHg): pressione arteriosa diastolica.
- SkinThickness (mm): spessore della pelle del tricipite, indicativo del grasso corporeo sottocutaneo.
- Insulin (µU/mL): livello di insulina nel sangue.
- BMI (kg/m²): indice di massa corporea, utile per valutare il peso corporeo rispetto all'altezza.
- DiabetesPedigreeFunction: funzione che stima la probabilità di sviluppare diabete in base alla storia familiare (valori più alti indicano una maggiore predisposizione genetica).
- Age (anni): età del paziente.
- Outcome: variabile target (0 = non diabetico, 1 = diabetico).

##### **Tipologia dei dati**
- Le feature sono numeriche continue e discrete.
- La variabile Outcome è categorica binaria (0/1).

##### **Obiettivo dell’analisi**
L’obiettivo del dataset è costruire un modello di classificazione binaria per predire la presenza di diabete in base ai valori clinici dei pazienti.

#### **Analisi del dataset**

##### **Distribuzione delle classi nel dataset**

L'analisi della distribuzione delle classi nel dataset è essenziale per comprendere eventuali squilibri, perciò procediamo con:

* `df["Outcome"].value_counts()` restituisce il conteggio di ogni valore unico presente nella colonna `Outcome` del DataFrame Pandas. Questo è utile per verificare la distribuzione delle classi nel dataset e **determinare se è bilanciato o meno**.
  - colonna `Outcome` del DataFrame rappresenta l'etichetta target, dove `1` indica la presenza di diabete, mentre `0` indica l'assenza di diabete.
  - `.value_counts()` conta il numero di occorrenze di ciascun valore unico presente nella colonna.
"""

df["Outcome"].value_counts()

"""* sns.countplot(x='Outcome', data=df):
  - si usa **seaborn**, una libreria per la visualizzazione dei dati, per creare un grafico a barre che conta la frequenza di ciascun valore nella colonna `Outcome`.
  - `x='Outcome'` specifica che i valori della colonna `Outcome` saranno visualizzati sull'asse x.
  - `data=df` specifica che i dati sono contenuti nel DataFrame `df`.

In altre parole, questo blocco di codice è usato per visualizzare graficamente la distribuzione delle classi nella colonna `Outcome` del dataset. Questa distribuzione rappresenta il bilanciamento tra i valori target, ossia `0` (assenza di diabete) e `1` (presenza di diabete).
"""

# Creiamo il grafico con conteggio delle classi
plt.figure(figsize=(8, 5))
ax = sns.countplot(x='Outcome', data=df, hue='Outcome', palette=['blue', 'red'], legend=False)

# Impostiamo titolo e assi
plt.title("Distribuzione delle Classi", size=14)
plt.xlabel("Outcome (0: Assenza, 1: Presenza di Diabete)")
plt.ylabel("Conteggio")

# Calcoliamo il numero totale di esempi
total = len(df)

# Aggiungiamo le etichette con la percentuale sopra ogni barra
for p in ax.patches:
    height = p.get_height()  # Altezza della barra
    if height > 0:
        percentage = '{:.1f}%'.format(100 * height / total)  # Percentuale
        ax.text(p.get_x() + p.get_width()/2., height + 5,  # Posizione del testo
                f'{height} ({percentage})',
                ha="center", fontsize=12, fontweight="bold", color="black")

# Mostriamo il grafico
plt.show()

"""Interpretazione:
  - Valore 0 (assenza di diabete): si verifica 500 volte nel dataset.
  - Valore 1 (presenza di diabete): si verifica 268 volte nel dataset. \

Dunque, questo mostra che **il dataset non è bilanciato**, perché la classe 0 (assenza di diabete) è molto più numerosa della classe 1 (presenza di diabete). Questa situazione può influenzare negativamente i modelli di machine learning, rendendoli meno efficaci nel predire la classe meno rappresentata.

##### **Dettagli del dataset**

Andando avanti con l'analisi dei dettagli del dataset, possiamo individuare diverse anomalie nei dati, perciò procediamo con:

- `df.describe()` mostra le statistiche principali di ogni feature numerica:
media, deviazione standard, minimo, massimo, quartili (25%, 50%, 75%).
Aiuta a individuare valori anomali (outlier) o distribuzioni non bilanciate.
"""

df.describe()

"""Conclusioni:
- I **valori 0** in Glucose, BloodPressure e BMI sono chiaramente **errori** e vanno sostituiti.
- SkinThickness e Insulin **potrebbero contenere** veri **valori nulli**, ma bisogna verificare più dettagliatamente le colonne.
- Gli altri dati **non mostrano anomalie** evidenti.

Per adesso stabiliamo che i **valori 0** in Glucose, BloodPressure e BMI devono essere trattati come **dati mancanti** e **gestiti nel preprocessing**.

Adesso analizziamo le statistiche per **pazienti SENZA diabete**:
"""

# Statistiche per pazienti SENZA diabete
df_no_diabetes = df[df['Outcome'] == 0]
print("Statistiche per pazienti senza diabete:")
df_no_diabetes.describe()

"""Conclusioni:
- L'insulina media è **anormalmente bassa**:
  - Il **25° percentile e il minimo sono 0**, il che suggerisce **errori nei dati**.
  - In un paziente sano, l'insulina a digiuno **non dovrebbe mai essere zero**!

- Molti valori di SkinThickness sono **pari a 0**:
  - Il **minimo è 0** e il **25° percentile è 0**, il che significa che **almeno il 25%** dei pazienti sani ha **SkinThickness = 0**.
  - Questo **è anormale**, poiché lo spessore cutaneo **non dovrebbe mai essere esattamente 0** negli individui sani.

- Altri parametri sembrano coerenti:
  - Glucose indica la media di 109.98, nella norma per non diabetici.
  - BloodPressure indica la media di 68.19, valore realistico.
  - BMI indica la media di 30.30, leggermente sopra la norma (sovrappeso).
  - Age indica la media di 31.19, con un intervallo tra 21 e 81 anni.

Questo significa che **i valori 0** di Insulin e SkinThickness **degli individui sani** devono essere trattati come **dati mancanti** e **gestiti nel preprocessing**.

Adesso analizziamo le statistiche per **pazienti CON diabete**:
"""

# Statistiche per pazienti CON diabete
df_diabetes = df[df['Outcome'] == 1]
print("\nStatistiche per pazienti con diabete:")
df_diabetes.describe()

"""Conclusioni:
- Molti valori di Insulin sono 0 per i diabetici:
  - Il **minimo è 0** e il **25° e 50° percentile sono 0**, il che significa che **almeno il 50%** dei pazienti diabetici ha **Insulin = 0**.
  - Questo **è anomalo**, perché nei pazienti diabetici (soprattutto quelli **con diabete di tipo 2**) i livelli di insulina sono generalmente **più alti** a causa dell'insulino-resistenza.
  - Tuttavia, se **la maggior parte dei dati** proviene da pazienti con **diabete di tipo 1**, allora è plausibile, poiché in questi soggetti il pancreas **non produce insulina** o ne **produce quantità minime**.

- Molti valori di SkinThickness sono 0 per i diabetici:
  - Il **minimo è 0** e il **25° percentile è 0**, il che significa che **almeno il 25%** dei pazienti diabetici ha **SkinThickness = 0**.
  - Questo è **potenzialmente un errore** nei dati, perché lo spessore cutaneo **non dovrebbe essere esattamente 0**.
  - Tuttavia, **è possibile** che alcuni pazienti diabetici abbiano effettivamente **un livello di grasso sottocutaneo molto basso**, soprattutto se **soffrono di distrofia lipoatrofica** o **perdita di massa grassa dovuta a diabete scarsamente controllato**.

- Altri parametri sembrano coerenti.

Dunque, scegliemo di **non gestire** i casi di **Insulin = 0** e **SkinThickness = 0** per gli **individui diabetici**.

##### **Statistiche del dataset**

Le statistiche di skewness (asimmetria) e kurtosis (curtosi) ci dicono quali variabili hanno una distribuzione non normale e necessitano di trasformazioni nella fase di preprocessing per migliorare la qualità del modello.

Procediamo con l'analisi della Skewness (Asimmetria):

- `skew()` indica quanto la distribuzione è sbilanciata a destra o a sinistra:
  - **Skewness > 0** significa la distribuzione sbilanciata **a destra** (coda più lunga a destra).
  - **Skewness < 0** significa la distribuzione sbilanciata **a sinistra** (coda più lunga a sinistra).
"""

# Selezioniamo solo le colonne numeriche (escludendo 'Outcome')
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Calcoliamo la skewness per le feature numeriche
print("Skewness (Asimmetria):")
df[numeric_columns].skew()

"""Conclusioni:
- Pregnancies e Age sono le variabili più **sbilanciate a destra**, quindi potremmo **applicare trasformazioni logaritmiche o sqrt** per renderle più normali.
- Glucose e BMI hanno una **lieve asimmetria**, quindi sono **quasi normali**.
- BloodPressure, SkinThickness e Insulin sono **quasi simmetriche** e **non necessitano di trasformazioni**.

Andando avanti con l'analisi della Kurtosis (Curtosi):
- `.kurtosis()` indica la forma della distribuzione rispetto alla normale:
  - **Kurtosis > 0** indica la distribuzione leptocurtica (più appuntita, più valori concentrati al centro e più code).
  - **Kurtosis < 0** indica la distribuzione platicurtica (più piatta, meno concentrata al centro).
"""

# Calcoliamo la kurtosis per le feature numeriche
print("\nKurtosis (Curtosi):")
df[numeric_columns].kurtosis()

"""Conclusioni:
- BloodPressure, BMI e Age sono più leptocurtiche (più **concentrate al centro** con code più pronunciate).
- Insulin e DiabetesPedigreeFunction sono molto platicurtiche (distribuzione **molto sparsa e appiattita**).
- SkinThickness e Glucose sono **leggermente** platicurtiche, ma **non in modo estremo**.

Cosa fare nel preprocessing:
- lavorare con le variabili con skewness elevata (fortemente asimmetriche), applicando una trasformazione:
  - Insulin (Skewness = 2.272, Kurtosis = 7.214)
  - DiabetesPedigreeFunction (Skewness = 1.919, Kurtosis = 5.594)

- lavorare con le variabili con kurtosis elevata (leptocurtiche) per controllare e trattare eventuali outlier:
  - BloodPressure (Kurtosis = 5.180) ha code molto lunghe, potrebbe contenere outlier.
  - BMI (Kurtosis = 3.290) ha tendenza leptocurtica, potrebbe contenere outlier.
  - Age (Kurtosis = 0.643) è leggermente leptocurtica, perciò non è necessaria una trasformazione, ma si può normalizzare.

---
---

### **Preprocessing del dataset**

#### **Gestione dei valori errati**

- `columns_to_fix` ontiene un elenco di colonne ('Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI') dove il valore 0 è considerato un errore o dato mancante.
  - `replace(0, np.nan)` sostituisce tutti i valori 0 con NaN (not a number) nelle colonne specificate.  
  - questo passo **è necessario** perché **un valore 0 non è realistico** in queste variabili (es. pressione sanguigna, glucosio, ecc.).
  - `df.isnull().sum()` conta i valori NaN in ogni colonna, permettendo di verificare quante celle risultano ora vuote dopo la sostituzione.
  - **output atteso** stampa il numero di valori nulli presenti nelle colonne dopo la sostituzione dei valori 0.
"""

# Definiamo le colonne da correggere per TUTTI i pazienti
columns_to_fix = ['Glucose', 'BloodPressure', 'BMI']

# Sostituiamo 0 con NaN nelle colonne critiche
df[columns_to_fix] = df[columns_to_fix].replace(0, np.nan)

# Correggiamo Insulin e SkinThickness SOLO per i pazienti sani (Outcome = 0)
df.loc[df['Outcome'] == 0, 'Insulin'] = df.loc[df['Outcome'] == 0, 'Insulin'].replace(0, np.nan)
df.loc[df['Outcome'] == 0, 'SkinThickness'] = df.loc[df['Outcome'] == 0, 'SkinThickness'].replace(0, np.nan)

# Contiamo i valori nulli dopo la sostituzione
print("Valori nulli dopo aver sostituito 0 con NaN:")
print(df.isnull().sum())

# Contiamo le righe con almeno un valore mancante nelle colonne critiche + Insulin + SkinThickness (solo per sani)
num_righe_con_null = df[columns_to_fix + ['Insulin', 'SkinThickness']].isnull().any(axis=1).sum()

print(f"Numero totale di righe con almeno un valore mancante: {num_righe_con_null}")

"""Abbiamo sostituito i valori 0 con NaN solo per:
 - **Glucose**:
  - Il glucosio nel sangue non può essere 0 in una persona viva. Un valore pari a 0 è un errore o un dato mancante.
 - **BloodPressure**:
  - Una pressione sanguigna di 0 mmHg significherebbe assenza di circolazione, quindi è un valore errato.
 - **BMI**:
  - Un BMI pari a 0 è irrealistico (implica un peso nullo), quindi è un errore nei dati.
 - **Insulin (solo per i pazienti sani)**:
  - Nei pazienti sani, un livello di insulina pari a 0 è altamente improbabile, poiché l'insulina è sempre presente nel sangue, anche in concentrazioni minime.
  - È probabile che i valori 0 rappresentino misurazioni mancanti e siano stati registrati erroneamente.
  - Per i pazienti diabetici, invece, 0 potrebbe essere un valore clinicamente plausibile in caso di diabete di tipo 1.
 - **SkinThickness (solo per i pazienti sani)**:
  - Lo spessore cutaneo non dovrebbe mai essere 0, a meno che non sia una misurazione non effettuata e registrata erroneamente.
  - Alcuni pazienti sani potrebbero avere livelli di grasso sottocutaneo molto bassi, ma 0 è poco realistico.
  - Per questo motivo, i valori 0 sono stati trattati come dati mancanti e saranno imputati nel preprocessing.

Una volta gestita la scelta delle colonne, si procede con l'imputazione dei valori mancanti (NaN) utilizzando un metodo avanzato, `IterativeImputer`, e verifica che tutti i valori mancanti siano stati gestiti.
  - IterativeImputer:
    - si tratta di un'imputazione avanzata in cui i valori mancanti vengono stimati in base ai valori delle altre feature nel dataset.
    - l'imputer utilizza un algoritmo iterativo per stimare i valori mancanti di ciascuna feature come una funzione lineare delle altre feature.
    - parametri utilizzati sono `max_iter=10` (specifica il numero massimo di iterazioni per stimare i valori) e `random_state=42` (garantisce la riproducibilità dei risultati).

  - Imputazione dei valori mancanti:
    - `fit_transform()` calcola i valori stimati per i NaN in base alle altre feature e li sostituisce nei dati originali.
    - i valori mancanti nelle colonne specificate in `columns_to_fix` vengono quindi riempiti con i valori stimati.

  - Controllo post-imputazione:
    - `df.isnull().sum()` conta i valori mancanti in ogni colonna dopo l'imputazione per assicurarsi che tutte le celle siano state correttamente riempite.
    - **output atteso** stampa del conteggio dei valori nulli dopo l'imputazione (sovrebbe risultare 0 valori nulli per tutte le colonne in columns_to_fix, indicando che tutti i valori mancanti sono stati sostituiti con stime).
"""

# Creiamo un IterativeImputer con limiti sui valori minimi
imputer = IterativeImputer(max_iter=10, random_state=42, min_value=0)  # min_value=0 impedisce valori negativi

# Applichiamo l'imputazione su tutte le colonne modificate
df[columns_to_fix + ['Insulin', 'SkinThickness']] = imputer.fit_transform(df[columns_to_fix + ['Insulin', 'SkinThickness']])

# Controllo post-imputazione
print("Valori nulli dopo IterativeImputer:")
print(df.isnull().sum())

"""Successivamente, verifichiamo di nuovo i dettagli del dataset per controllare che IterativeImputer abbia modificato correttamente i dati:"""

# Statistiche per pazienti sia SENZA diabete che CON diabete
df.describe()

# Statistiche per pazienti SENZA diabete
df_no_diabetes = df[df['Outcome'] == 0]
print("Statistiche per pazienti senza diabete:")
df_no_diabetes.describe()

# Statistiche per pazienti CON diabete
df_diabetes = df[df['Outcome'] == 1]
print("\nStatistiche per pazienti con diabete:")
df_diabetes.describe()

"""#### **Gestione degli outlier**

Questo pezzo di codice applica un PowerTransformer per trasformare le variabili asimmetriche (con distribuzioni non normali) in distribuzioni più vicine a una normale (gaussiane). Questo passaggio è utile per migliorare le prestazioni di molti algoritmi di machine learning che assumono dati con distribuzioni normali.

 Tuttavia, prima di applicare la trasformazione, mettiamo un'esempio delle distribuzioni delle variabili Insulin e DiabetesPedigreeFunction che mostrano una forte asimmetria:
"""

# Visualizzazione delle distribuzioni originali (prima della trasformazione)
plt.figure(figsize=(12, 6))
for i, col in enumerate(['Insulin', 'DiabetesPedigreeFunction']):
    plt.subplot(1, 2, i + 1)
    sns.kdeplot(data=df, x=col, fill=True)
    plt.title(f"Distribuzione originale: {col}")
plt.tight_layout()
plt.show()

"""Adesso applichiamo la trasformazione:
  - `PowerTransformer()`:
    - è uno strumento di preprocessing di scikit-learn che applica trasformazioni non lineari.
    - serve per stabilizzare la varianza, rendere la distribuzione dei dati più simmetrica, migliorare le prestazioni degli algoritmi sensibili alla distribuzione dei dati, come la regressione lineare o i modelli basati su distanza (es. SVM).
  - `fit_transform()`:
    - calcola i parametri della trasformazione in base ai dati forniti, poi applica la trasformazione ai dati.
    - di default utilizza il metodo Yeo-Johnson, che supporta dati sia positivi che negativi (al contrario del Box-Cox che funziona solo con valori positivi).
"""

# Applicazione di PowerTransformer per variabili asimmetriche
power_transformer = PowerTransformer()
df[['Insulin', 'DiabetesPedigreeFunction']] = power_transformer.fit_transform(df[['Insulin', 'DiabetesPedigreeFunction']])

"""Infatti, dopo l'applicazione del `PowerTransformer` si vede che la distribuzione si è stabilizzata:
- Insulin:
    - la distribuzione si avvicina a una forma simmetrica e centrata, migliorando la normalità.
- DiabetesPedigreeFunction:
    - anche questa distribuzione diventa più simmetrica e più vicina a una distribuzione normale.

"""

# Visualizzazione delle distribuzioni trasformate (dopo PowerTransformer)
plt.figure(figsize=(12, 6))
for i, col in enumerate(['Insulin', 'DiabetesPedigreeFunction']):
    plt.subplot(1, 2, i + 1)
    sns.kdeplot(data=df, x=col, fill=True)
    plt.title(f"Distribuzione trasformata: {col}")
plt.tight_layout()
plt.show()

"""Questa trasformazione implica:
1. Rimozione dell'asimmetria:
  - algoritmi come la regressione lineare, le reti neurali o i modelli basati su distanze funzionano meglio con dati normalizzati.

2. Stabilità numerica:
  - valori estremi vengono ridotti, diminuendo il rischio di influenze sproporzionate da outlier.

3. Miglioramento dell'accuratezza del modello:
  - molti modelli producono risultati più stabili e interpretabili con dati normalizzati.
"""

# Lista delle colonne da visualizzare
columns_to_check = ['BloodPressure', 'BMI', 'Age', 'Insulin', 'DiabetesPedigreeFunction']

# Impostiamo il layout con due righe
rows = 2
cols = (len(columns_to_check) + 1) // rows  # Arrotondamento per gestire il numero di colonne

plt.figure(figsize=(12, 8))  # Adattiamo la dimensione per una visualizzazione chiara

# Creazione dei boxplot su due righe
for i, col in enumerate(columns_to_check):
    plt.subplot(rows, cols, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot di {col}')

plt.tight_layout()
plt.show()

"""Adesso proviamo di individuare outlier con boxplot per le variabili di BloodPressure, BMI (Kurtosis = 3.290) e Age (Kurtosis = 0.643):"""

# Creazione boxplot per individuare gli outlier
plt.figure(figsize=(12, 5))
columns_to_check = ['BloodPressure', 'BMI', 'Age']
for i, col in enumerate(columns_to_check):
    plt.subplot(1, 3, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot di {col}')

plt.tight_layout()
plt.show()

"""Osservazioni:
- BloodPressure:
  - Ci sono alcuni outlier nella parte inferiore (valori inferiori a 40 mmHg).
  - La maggior parte dei valori si concentra tra 60 e 80 mmHg, con una distribuzione abbastanza concentrata.
  - I valori estremamente bassi potrebbero rappresentare **errori di inserimento** o **casi rari** (es. ipotensione grave)..

- BMI:
  - Ci sono outlier nella parte superiore (valori sopra 50).
  - La maggior parte dei valori è concentrata tra 25 e 40, che è comune in un dataset che include pazienti diabetici (sovrappeso o obesi).
  - I valori molto alti (> 50) potrebbero rappresentare **errori di inserimento** o **casi rari**.

- Age:
  - Gli outlier sono nella parte superiore, con alcuni valori sopra i 70 anni.
  - La maggior parte dei valori è distribuita tra 20 e 50 anni, che è tipico di un dataset clinico.
  - **Non è necessario rimuovere gli outlier** se sono plausibili, poiché un'età elevata è coerente con il rischio di diabete.

Quindi, procediamo con:
- Calcolo dell'IQR (Interquartile Range):
  - L'IQR misura la dispersione dei dati tra il 25° e il 75° percentile, che rappresenta il range intermedio della distribuzione.
- Identificazione degli outlier:
  - I valori al di sotto di `Q1 - 1.5 * IQR` o al di sopra di `Q3 + 1.5 * IQR` sono considerati anomalie (outlier).
- Filtraggio del dataset:
  - Vengono mantenuti solo i valori che rientrano nel range accettabile.
  - Gli outlier vengono rimossi dal dataset, lasciando solo i valori compresi tra i limiti calcolati.
"""

# Lista delle colonne da trattare
columns_to_fix = ['BloodPressure', 'BMI']

for col in columns_to_fix:
    # Calcolo IQR per ogni colonna
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    # Rimuoviamo gli outlier per la colonna corrente
    df = df[(df[col] >= Q1 - 1.5 * IQR) & (df[col] <= Q3 + 1.5 * IQR)]

"""Adesso possiamo verificare cosa è stato effettivamente cambiato:"""

# Creazione boxplot aggiornati
plt.figure(figsize=(12, 5))
columns_to_check = ['BloodPressure', 'BMI', 'Age']
for i, col in enumerate(columns_to_check):
    plt.subplot(1, 3, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot aggiornato di {col}')

plt.tight_layout()
plt.show()

"""Conclusione:
- BloodPressure e BMI:
  - Gli outlier sono stati trattati correttamente e le distribuzioni ora appaiono più pulite e rappresentative.

- Age:
  - Gli outlier plausibili sono stati mantenuti e la distribuzione non è stata alterata.

#### **Standardizzazione delle feature**

Questo pezzo di codice esegue la standardizzazione delle feature, un passaggio fondamentale del preprocessing per i modelli di machine learning.
  Tuttavia, prima di applicare la standardizzazione, mettiamo un'esempio delle distribuzioni attuali:
"""

# Escludiamo solo il target, mantenendo tutte le feature originali
X_original = df.drop(columns=["Outcome"])

# Calcolo dinamico delle righe necessarie per la disposizione dei subplot
n_cols = 3  # Numero di colonne nei subplot
n_rows = int(np.ceil(len(X_original.columns) / n_cols))  # Calcolo del numero di righe necessario

# Visualizzazione degli istogrammi delle feature originali
plt.figure(figsize=(12, 8))
for i, col in enumerate(X_original.columns):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.histplot(data=X_original, x=col, bins=30, kde=False)  # Istogramma senza KDE
    plt.title(f"Istogramma: {col}")

plt.suptitle('Distribuzione delle Feature Originali', fontsize=16)
plt.tight_layout()
plt.show()

# Escludiamo solo il target, mantenendo tutte le feature originali
X_original = df.drop(columns=["Outcome"])

# Calcolo dinamico delle righe necessarie per la disposizione dei subplot
n_cols = 3  # Numero di colonne nei subplot
n_rows = int(np.ceil(len(X_original.columns) / n_cols))  # Calcolo del numero di righe necessario

# Visualizzazione delle distribuzioni delle feature originali
plt.figure(figsize=(12, 8))
for i, col in enumerate(X_original.columns):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.kdeplot(data=X_original, x=col, fill=True)
    plt.title(f"Distribuzione: {col}")

plt.suptitle('Distribuzione delle Feature Originali', fontsize=16)
plt.tight_layout()
plt.show()

"""Adesso applichiamo la standardizzazione e vediamo cosa succede nel dettaglio:
  - `StandardScaler` trasforma i dati in modo che ogni feature abbia **media 0** e **deviazione standard 1**.
  - `X = df.drop(columns=["Outcome"])` esclude la variabile target (Outcome) dal dataset, poiché questa non deve essere standardizzata (è il valore da prevedere, non da trasformare).
  - `fit_transform(X)` calcola la media e la deviazione standard di ciascuna colonna nel dataset X, poi utilizza questi parametri per trasformare ogni valore di ciascuna feature secondo la formula della standardizzazione.
  - output atteso è un array numpy in cui tutte le feature sono standardizzate (media 0 e deviazione standard 1).

E' importante standardizzare le feature, perchè le feature nel dataset spesso hanno scale diverse (es. Glucose varia da 50 a 200, mentre Age varia da 20 a 80). Standardizzare elimina il problema delle scale diverse.
  
Tra altro, esistono anche modelli sensibili alla scala, come SVM, k-NN, regressione logistica, reti neurali e ecc. Questi algoritmi calcolano distanze o ottimizzano con gradienti e richiedono che le feature abbiano valori comparabili.
"""

# Standardizzazione di tutte le feature
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_original)

"""Dopo aver applicato la standardizzazione con StandardScaler, che centra i dati a una media di 0 e una deviazione standard di 1, il codice visualizza le distribuzioni delle feature standardizzate."""

# Creiamo un nuovo DataFrame standardizzato solo con le feature originali
X_scaled_df = pd.DataFrame(X_scaled, columns=X_original.columns)

# Calcolo dinamico delle righe necessarie per la visualizzazione
n_cols = 3  # Numero di colonne nei subplot
n_rows = int(np.ceil(len(X_scaled_df.columns) / n_cols))  # Calcola il numero di righe necessarie

# Visualizzazione delle distribuzioni standardizzate delle feature originali
plt.figure(figsize=(12, 8))
for i, col in enumerate(X_scaled_df.columns):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.kdeplot(data=X_scaled_df, x=col, fill=True)
    plt.title(f"Distribuzione Standardizzata: {col}")

plt.suptitle('Distribuzione delle Feature Standardizzate', fontsize=16)
plt.tight_layout()
plt.show()

"""Dai grafici si vede come le distribuzioni siano state ridimensionate e centrate, garantendo che tutte le variabili siano sulla stessa scala, essenziale per il corretto funzionamento di molti algoritmi di machine learning.

#### **Bilanciamento del dataset con SMOTE**

Questo pezzo di codice applica una tecnica per bilanciare i dataset sbilanciati.
- genera campioni sintetici per la classe minoritaria (nel nostro caso, Outcome=1, ovvero i pazienti diabetici) anziché duplicare semplicemente i dati esistenti.
- questo approccio aiuta a evitare problemi di overfitting che possono verificarsi quando si duplicano i dati reali.
"""

# Creazione del target y
y = df["Outcome"]

# Controlliamo che X_scaled_df e y abbiano le stesse dimensioni
print(f"Dimensioni di X_scaled_df: {X_scaled_df.shape}")
print(f"Dimensioni di y: {y.shape}")

# Applicazione di SMOTE per bilanciare il dataset
smote = SMOTE(sampling_strategy="auto", random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled_df, y)

# Controlliamo la nuova distribuzione delle classi dopo SMOTE
print("Distribuzione delle classi dopo SMOTE:")
print(y_resampled.value_counts())

"""Una volta abbiamo utilizzato la funziona SMOTE, procediamo con:
* `y_resampled.value_counts()` restituisce il conteggio di ogni valore unico presente nella colonna `Outcome` del DataFrame Pandas. Questo è utile per verificare la distribuzione delle classi nel dataset e **determinare se è bilanciato o meno**.
  - colonna `Outcome` del DataFrame rappresenta l'etichetta target, dove `1` indica la presenza di diabete, mentre `0` indica l'assenza di diabete.
  - `.value_counts()` conta il numero di occorrenze di ciascun valore unico presente nella colonna.

"""

y_resampled.value_counts()

"""Interpretazione:
  - Valore 0 (assenza di diabete): si verifica 490 volte nel dataset.
  - Valore 1 (presenza di diabete): si verifica 490 volte nel dataset. \

Dunque, questo mostra che il dataset è bilanciato, perché la classe 0 (assenza di diabete) ha lo stesso numero di casi della classe 1 (presenza di diabete).

* sns.countplot(x='Outcome', data=df):
  - si usa **seaborn**, una libreria per la visualizzazione dei dati, per creare un grafico a barre che conta la frequenza di ciascun valore nella colonna `Outcome`.
  - `x='Outcome'` specifica che i valori della colonna `Outcome` saranno visualizzati sull'asse x.
  - `data=df` specifica che i dati sono contenuti nel DataFrame `df`.

In altre parole, questo blocco di codice è usato per visualizzare graficamente la distribuzione delle classi nella colonna `Outcome` del dataset. Questa distribuzione rappresenta il bilanciamento tra i valori target, ossia `0` (assenza di diabete) e `1` (presenza di diabete).
"""

# Convertiamo y_resampled in un DataFrame per compatibilità con seaborn
y_resampled_df = pd.DataFrame({'Outcome': y_resampled})

# Creiamo il grafico con conteggio delle classi
plt.figure(figsize=(8, 5))
ax = sns.countplot(x='Outcome', data=y_resampled_df, hue='Outcome', palette=['blue', 'red'], legend=False)

# Impostiamo titolo e assi
plt.title("Distribuzione delle Classi (Outcome) dopo SMOTE", size=14)
plt.xlabel("Outcome (0: Assenza, 1: Presenza di Diabete)")
plt.ylabel("Conteggio")

# Calcoliamo il numero totale di esempi dopo SMOTE
total = len(y_resampled_df)

# Aggiungiamo le etichette con la percentuale sopra ogni barra
for p in ax.patches:
    height = p.get_height()  # Altezza della barra
    if height > 0:
        percentage = '{:.1f}%'.format(100 * height / total)  # Percentuale
        ax.text(p.get_x() + p.get_width()/2., height + 5,  # Posizione del testo
                f'{height} ({percentage})',
                ha="center", fontsize=12, fontweight="bold", color="black")

# Mostriamo il grafico
plt.show()

"""---
---

### **Grafici e matrice di correlazione**

#### **Grafici**

Questo pezzo di codice crea dei grafici di densità (KDE - Kernel Density Estimation) per ogni feature del dataset. I grafici mostrano la distribuzione probabilistica delle variabili, aiutando a visualizzare la forma dei dati e la loro variazione.
"""

# Creiamo un DataFrame con solo le feature originali dopo preprocessing
X_resampled_original = pd.DataFrame(X_resampled, columns=X_original.columns)

# KDE per il dataset con solo le feature originali dopo preprocessing con più spazio tra i grafici
plt.figure(figsize=(12, 8))

for i, col in enumerate(X_resampled_original.columns):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.kdeplot(data=X_resampled_original, x=col, fill=True)
    plt.title(col)

plt.suptitle('Distribuzione delle Feature Preprocessati', fontsize=16)
plt.tight_layout()
plt.show()

"""#### **Matrice di correlazione**

Questo pezzo di codice visualizza una matrice di correlazione tra le feature numeriche nel dataset, rappresentata visivamente come una heatmap:
- ogni cella rappresenta la correlazione tra due variabili.
- colori diversi indicano l'intensità della correlazione.
"""

# Matrice di correlazione con le feature originali
plt.figure(figsize=(10, 8))
sns.heatmap(X_resampled_original.corr(), annot=True, cmap='coolwarm')
plt.title('Matrice di Correlazione')
plt.show()

"""Osservazioni generali:
1. Colore e valore della correlazione:
  - **blu scuro** indica correlazioni **deboli** o **negative**.
  - **rosso** più intenso indica correlazioni **forti** e **positive**.
  - valori **vicini a 1 o -1** indicano una correlazione **forte**.
  - valori **vicini a 0** indicano una correlazione **debole** o **assente**.

2. La diagonale principale (valore 1):
  - rappresenta la correlazione di ogni variabile **con se stessa** (sempre 1).

Relazioni chiave tra le variabili:
  - SkinThickness e Insulin (0.53):
    - Esiste una correlazione moderata positiva, suggerendo che **un aumento dello spessore della pelle** (SkinThickness) **è associato a un aumento dei livelli di insulina**. Questo è coerente con la fisiologia, in quanto livelli elevati di insulina sono spesso legati a un maggiore accumulo di grasso sottocutaneo.

  - BMI e SkinThickness (0.35):
    - Esiste una correlazione moderata positiva, che riflette **la relazione tra l'indice di massa corporea e il grasso corporeo** misurato dal "SkinThickness".

  - Pregnancies e Age (0.52):
    - Esiste una correlazione positiva significativa, che indica che **le donne con un'età maggiore tendono ad avere più gravidanze**, un **risultato atteso**.

  - Glucose e BMI (0.25):
    - Esiste una correlazione debole positiva, suggerendo che un **maggiore livello di glucosio è leggermente associato a un BMI più alto**.

  - BloodPressure e BMI (0.29):
    - Esiste una correlazione debole positiva, che potrebbe indicare che **un aumento del BMI è associato a un lieve aumento della pressione sanguigna**.

Di conseguenza possiamo tratte che variabili come **Glucose**, **BMI** e **Insulin**, che mostrano **correlazioni significative con altre variabili**, potrebbero avere **un impatto maggiore sulla classificazione del diabete**.

---
---

### **Analisi delle Componenti Principali (PCA)**

Questo codice applica l'analisi delle componenti principali (PCA) per ridurre la dimensionalità dei dati mantenendo la maggior parte dell'informazione possibile. Generalmente serve per:
- Ridurre la complessità del modello:
  - Dataset con molte feature possono aumentare la complessità computazionale e introdurre problemi come l'overfitting.
  - PCA riduce il numero di feature trasformandole in componenti principali che sono combinazioni lineari delle feature originali, semplificando il modello.
- Eliminare la ridondanza nei dati:
  - In molti dataset, alcune feature possono essere altamente correlate tra loro, il che introduce ridondanza.
  - PCA identifica le direzioni di massima varianza, combinando le feature correlate in singole componenti principali.
- Mantenere la maggior parte dell'informazione:
  - Non tutte le feature contribuiscono in modo significativo alla varianza totale del dataset.
  - PCA ordina le componenti principali in base alla loro capacità di spiegare la varianza, permettendo di selezionare solo quelle che contengono la maggior parte dell'informazione.
- Migliorare le performance dei modelli:
  -  Alcuni modelli di machine learning funzionano meglio con un numero ridotto di feature e quando le feature non sono troppo correlate.
  - PCA trasforma le feature originali in componenti ortogonali (non correlate), migliorando la qualità dei dati per i modelli che assumono indipendenza delle variabili.

Funzionamento del codice:
1. Applicazione di PCA:
  - Il dataset originale viene trasformato in un nuovo spazio definito dalle componenti principali, che sono combinazioni lineari delle feature originali.
  - `X_pca` è il dataset trasformato nello spazio delle componenti principali.

2. Varianza spiegata:
  - Viene calcolata la proporzione di varianza spiegata da ciascuna componente principale rispetto alla varianza totale.
  - `explained_variance` è un array che mostra quanto ogni componente contribuisce alla spiegazione della varianza.

3. Varianza spiegata cumulativa:
  - Viene calcolata la somma cumulativa della varianza spiegata dalle componenti principali.
  - `cumulative_variance` permette di capire quante componenti principali sono necessarie per spiegare una certa percentuale della varianza totale.

4. Creazione dei grafici:
  - Grafico della varianza spiegata per ogni componente:
    - Viene creato un grafico a linee che mostra la varianza spiegata da ciascuna componente.
    - Lo scopo è **identificare quali componenti contribuiscono maggiormente alla spiegazione della varianza**.

  - Scree Plot (varianza cumulativa e varianza spiegata):
    - Viene combinata un **istogramma** (scree plot) che mostra la varianza spiegata da ciascuna componente e una **curva** che rappresenta la varianza spiegata cumulativa.
"""

# Applicare PCA mantenendo tutte le componenti principali
pca = PCA(n_components=len(X_resampled.columns))
X_pca = pca.fit_transform(X_resampled)

# Ottenere la varianza spiegata da ogni componente
explained_variance = pca.explained_variance_ratio_

# Calcolare la varianza spiegata cumulativa
cumulative_variance = np.cumsum(explained_variance)

# Creazione dei due grafici: Varianza spiegata e Scree Plot
plt.figure(figsize=(12, 6))

# Grafico della varianza spiegata per ogni componente
plt.subplot(1, 2, 1)
plt.plot(range(1, pca.n_components_ + 1), explained_variance, marker='o', linestyle='--', color='darkred')

# Aggiungere annotazioni sopra ogni punto
for i, value in enumerate(explained_variance):
    plt.text(i + 1, value, f'{value * 100:.2f}%', ha='center', va='bottom', fontsize=9)

plt.xlabel('Componenti della PCA')
plt.ylabel('Varianza spiegata')
plt.title("Varianza spiegata per ogni componente")
plt.grid()

# Scree Plot con varianza cumulativa
plt.subplot(1, 2, 2)
plt.bar(range(1, pca.n_components_ + 1), explained_variance, color='blue', alpha=0.5, align='center')
plt.plot(range(1, pca.n_components_ + 1), cumulative_variance, marker='o', color='red')

# Annotazioni delle percentuali cumulative
for i, value in enumerate(cumulative_variance):
    plt.text(i + 1, value, f'{value * 100:.2f}%', ha='center', va='bottom', fontsize=9)

plt.xlabel('Componenti della PCA')
plt.ylabel('Varianza spiegata / Varianza cumulativa')
plt.title("Scree Plot della PCA")

plt.tight_layout()
plt.show()

"""Conclusioni:
- Contributo delle componenti principali:
  - La prima componente spiega il 24.56% della varianza nei dati.
  - Le prime due componenti insieme spiegano circa il 47.16% della varianza.
  - Le prime cinque componenti spiegano cumulativamente circa l'83.11% della varianza totale, un valore sufficiente per rappresentare la maggior parte delle informazioni originali.

- Scree Plot:
  - La curva della varianza cumulativa si appiattisce significativamente dopo la quinta componente, indicando che le componenti successive aggiungono un contributo limitato in termini di varianza spiegata.
  - È possibile ridurre la dimensionalità del dataset mantenendo 5 componenti principali, preservando così la maggior parte delle informazioni.

- Varianza spiegata:
  - Le componenti successive alla quinta contribuiscono poco alla varianza (meno del 10% ciascuna).
  - Le ultime componenti (ad esempio la settima e l'ottava) sono quasi trascurabili in termini di varianza spiegata.

Queste conclusioni supportano la scelta di **ridurre le dimensioni del dataset** considerando **solo le prime 5 componenti principali**, ottimizzando il bilanciamento tra semplicità del modello e conservazione delle informazioni.
"""

fig, axes = plt.subplots(2, 3, figsize=(18, 10))  # 2 righe, 3 colonne
components = [(0, 1), (1, 2), (2, 3), (3, 4), (0, 4)]  # 5 coppie di componenti da visualizzare
unique_labels = np.unique(y_resampled)  # Trova le classi uniche (0 e 1)

scatter_plots = []  # Per raccogliere gli scatter plot per la legenda

for i, (pc_x, pc_y) in enumerate(components):
    ax = axes[i // 3, i % 3]  # Posizione nella griglia
    for label in unique_labels:
        mask = (y_resampled == label).values
        scatter = ax.scatter(X_pca[mask, pc_x], X_pca[mask, pc_y],
                             color='blue' if label == 0 else 'red',
                             label='No Diabete' if label == 0 else 'Diabete')
        if i == 0:  # Aggiungiamo solo una volta i plot alla lista per la legenda
            scatter_plots.append(scatter)

    ax.set_xlabel(f'Componente {pc_x + 1}')
    ax.set_ylabel(f'Componente {pc_y + 1}')
    ax.set_title(f"PC{pc_x + 1} vs. PC{pc_y + 1}")

# Rimuoviamo l'ultimo subplot vuoto
fig.delaxes(axes[1, 2])

plt.tight_layout()
fig.legend(handles=scatter_plots, labels=["No Diabete", "Diabete"], loc="upper right")
plt.show()

"""Tuttavia, dai grafici emerge una sovrapposizione significativa tra le classi No Diabete e Diabete, suggerendo che le componenti principali catturano la varianza dei dati, ma non garantiscono una separazione netta tra le due categorie.

Questa sovrapposizione indica che:

- Le informazioni utili per distinguere le classi non sono concentrate solo nelle prime due o tre componenti, ma probabilmente sono distribuite su più dimensioni.
- La PCA riduce la dimensionalità mantenendo una parte significativa della varianza, ma potrebbe non essere sufficiente per migliorare la separabilità tra le classi in un modello di classificazione.

In sintesi, sebbene la PCA aiuti a rappresentare i dati in uno spazio a dimensioni ridotte, i risultati mostrano che non è la strategia ottimale per separare le classi in questo caso.
"""

fig = plt.figure(figsize=(12, 10))

# PC1, PC2, PC3
ax1 = fig.add_subplot(221, projection='3d')
# PC2, PC3, PC4
ax2 = fig.add_subplot(222, projection='3d')
# PC3, PC4, PC5
ax3 = fig.add_subplot(223, projection='3d')

for label in unique_labels:
    mask = (y_resampled == label).values

    ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], X_pca[mask, 2],
                color='blue' if label == 0 else 'red', label='No Diabete' if label == 0 else 'Diabete')

    ax2.scatter(X_pca[mask, 1], X_pca[mask, 2], X_pca[mask, 3],
                color='blue' if label == 0 else 'red', label='No Diabete' if label == 0 else 'Diabete')

    ax3.scatter(X_pca[mask, 2], X_pca[mask, 3], X_pca[mask, 4],
                color='blue' if label == 0 else 'red', label='No Diabete' if label == 0 else 'Diabete')

# Etichette degli assi
ax1.set_xlabel('PC1'), ax1.set_ylabel('PC2'), ax1.set_zlabel('PC3')
ax2.set_xlabel('PC2'), ax2.set_ylabel('PC3'), ax2.set_zlabel('PC4')
ax3.set_xlabel('PC3'), ax3.set_ylabel('PC4'), ax3.set_zlabel('PC5')

ax1.set_title("PC1, PC2, PC3")
ax2.set_title("PC2, PC3, PC4")
ax3.set_title("PC3, PC4, PC5")
plt.tight_layout()
plt.legend()
plt.show()

"""I grafici 3D delle componenti principali (PC1-PC5) confermano l'osservazione precedente: le classi "No Diabete" e "Diabete" risultano ancora parzialmente sovrapposte, indicando che la PCA non fornisce una chiara separazione tra le due categorie.

Di nota che:
- La PCA aiuta a ridurre la dimensionalità, ma non è sufficiente per separare chiaramente le classi.
- Le prime componenti catturano una parte della separabilità, ma non in modo ottimale.
- L’utilizzo di tutte le feature originali potrebbe essere più efficace per il modello di classificazione

L'ultimo controllo effettuato ha lo scopo di confrontare le prestazioni del modello di classificazione con e senza PCA, valutando l’impatto della riduzione dimensionale sull'accuratezza.

Il codice implementa:

- Un modello senza PCA, che utilizza tutte le feature originali.
- Un modello con PCA, in cui le feature vengono ridotte a 5 componenti principali.
Dopo l’addestramento, entrambi i modelli vengono valutati sul Validation Set, per determinare quale approccio garantisce le migliori prestazioni.
"""

# Prima suddivisione: Training (70%) e temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Seconda suddivisione: Validation (15%) e Test (15%)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Applichiamo PCA dopo la suddivisione, solo ai set di feature
pca = PCA(n_components=5)
X_train_pca = pca.fit_transform(X_train)
X_val_pca = pca.transform(X_val)
X_test_pca = pca.transform(X_test)

# Addestriamo il modello SENZA PCA
clf_original = DecisionTreeClassifier(random_state=42)
clf_original.fit(X_train, y_train)

# Valutazione sul Validation Set
y_pred_val_original = clf_original.predict(X_val)
acc_val_original = accuracy_score(y_val, y_pred_val_original)

# Valutazione sul Test Set
y_pred_test_original = clf_original.predict(X_test)
acc_test_original = accuracy_score(y_test, y_pred_test_original)

# Addestriamo il modello CON PCA
clf_pca = DecisionTreeClassifier(random_state=42)
clf_pca.fit(X_train_pca, y_train)

# Valutazione sul Validation Set con PCA
y_pred_val_pca = clf_pca.predict(X_val_pca)
acc_val_pca = accuracy_score(y_val, y_pred_val_pca)

# Valutazione sul Test Set con PCA
y_pred_test_pca = clf_pca.predict(X_test_pca)
acc_test_pca = accuracy_score(y_test, y_pred_test_pca)

# Confronto delle prestazioni sui set di validazione e test
print(f"Accuratezza Validation Set senza PCA: {acc_val_original:.4f}")
print(f"Accuratezza Validation Set con PCA: {acc_val_pca:.4f}")

print(f"Accuratezza Test Set senza PCA: {acc_test_original:.4f}")
print(f"Accuratezza Test Set con PCA: {acc_test_pca:.4f}")

"""Conclusione definitiva sull'uso della PCA:
- **Il modello senza PCA è più preciso**:
  - L’accuratezza sul Validation Set è più alta senza PCA (93.2%), indicando che il modello con tutte le feature cattura meglio l'informazione.
  - Anche sul Test Set il modello senza PCA è leggermente migliore (89.1% vs. 88.4%), suggerendo che la PCA ha rimosso parte dell’informazione utile.

- **La PCA riduce l’accuratezza, ma stabilizza leggermente il modello**:
  - Il modello con PCA ha una minore variazione tra Validation e Test Set (da 87.8% a 88.4%), mentre senza PCA c'è una diminuzione più evidente (da 93.2% a 89.1%).
  - Questo potrebbe indicare che PCA aiuta leggermente a ridurre l’overfitting, ma a discapito della performance complessiva.

- **La PCA non è la strategia ottimale** in questo caso:
  - Poiché il modello senza PCA ha sempre un’accuratezza superiore, significa che l’uso di tutte le feature è più efficace nel distinguere le classi.
  - PCA non è necessaria in questo caso, dato che **il dataset non ha troppe feature** e **l’analisi della matrice di correlazione ha mostrato che molte variabili non erano fortemente collegate**.

---
---

### **Modelli di classificazione**

#### **Divisione del dataset in Training, Validation e Test Set**

Una volta finita la fase di preprocessing, è necessario dividere i dati in training set, validation set e test set per addestrare e valutare i modelli di Machine Learning.

Utilizziamo la funzione `train_test_split()` per suddividere il dataset in:
- 70% Training – 15% Validation – 15% Test:
  - Questa suddivisione consente di **mantenere un validation set separato** per ottimizzare il modello **prima del test** finale.
  - Il training set viene usato per **l’addestramento del modello**, il validation set per **la selezione dell’ipermodello** e il test set per **la valutazione finale**.
  
In questo caso, il modello viene addestrato con una percentuale equilibrata di dati per training e validazione, garantendo una valutazione finale accurata sul test set.
"""

# Prima suddivisione: Training (70%) e temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Seconda suddivisione: Validation (15%) e Test (15%)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Controllo delle dimensioni
print(f"Train Set: {X_train.shape}")
print(f"Validation Set: {X_val.shape}")
print(f"Test Set: {X_test.shape}")

"""#### **Valutazione delle performance e della generalizzazione del modello**

Durante l’addestramento dei modelli, utilizzeremo le **metriche standard** per il classification report:
- Accuracy:
  - visualizza **il percentuale** di predizioni corrette.
- Precision:
  - misura **la qualità** delle predizioni positive.
- Recall:
  - valuta **la capacità del modello di catturare** tutte le istanze positive.
- F1-score:
  - visualizza **la media armonica tra Precision e Recall**, utile per dataset sbilanciati.

Tuttavia, valutare un modello esclusivamente su un singolo training, validation e test set potrebbe non essere sufficiente per garantirne la robustezza. Per questo motivo, viene introdotta **la cross-validation** come strumento fondamentale per stimare in modo più approfondito e affidabile **le performance medie** del modello **su dati non visti**.

##### **Cross-validation per una valutazione avanzata**

1.  Si definisce un dizionario `scoring` che contiene le metriche di performance (accuracy, precision, recall e f1_score) per valutare il modello.   Queste metriche vengono calcolate con la media "macro" per tener conto della media dei punteggi per ogni classe in modo equilibrato.

2. Si definisce una funzione per eseguire la cross-validation:
  - Prende in input il **modello** da addestrare, le **caratteristiche** (`X`), le **etichettature** (`y`) e un parametro opzionale per **il numero di fold** di cross-validation.
  - La **StratifiedKFold** viene utilizzata per suddividere il dataset **in 10 fold**.
  - Per ogni fold, vengono calcolate le metriche definite in `scoring`.
  - I punteggi per ciascuna metrica vengono salvati in un dizionario `results`.
  - Alla fine, vengono **stampati i valori** per ciascun fold, insieme alla **media** e all'**intervallo di confidenza al 95%** per ogni metrica.

3. Si definisce una funzione per calcolare l'intervallo di confidenza al 95% per i valori di ciascuna metrica:
  - Viene utilizza la distribuzione **t di Student** per calcolare l'intervallo, che è appropriato quando si ha una **distribuzione di campioni limitata**, come nel caso dei fold di cross-validation.
  - La funzione riceve **un array di punteggi** e restituisce **l'intervallo di confidenza arrotondato a 6 decimali**.
"""

# Preparazione per la cross-validation avanzata con metriche multiple
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, average='macro'),
    'recall': make_scorer(recall_score, average='macro'),
    'f1_score': make_scorer(f1_score, average='macro')
}

print("Misure utilizzate durante cross validation:", ' '.join(scoring.keys()))

# Funzione per eseguire la cross-validation con metriche avanzate
def my_cross_validation(model, X, y, cv):
    scores = cross_validate(model, X, y, cv=cv, scoring=scoring)

    results = {
        'accuracy': scores['test_accuracy'],
        'precision': scores['test_precision'],
        'recall': scores['test_recall'],
        'f1-score': scores['test_f1_score']
    }

    print("Cross-validation eseguita con 10 fold:")
    # Stampa dei risultati per ogni metrica
    for metric, values in results.items():
        print(f"--- {metric.capitalize()} di ogni fold, con media pari a {np.mean(values):.8f} ---\n")
        print(values)
        conf_interval = confidence_interval(values)
        print(f"\nIntervallo di confidenza 95%: {conf_interval}\n")

    return results

print("Funzione my_cross_validation ora disponibile.")


# Funzione per calcolare gli intervalli di confidenza con 6 decimali
def confidence_interval(scores):
    # Calcola la media e l'errore standard dei punteggi
    mean_val = np.mean(scores)
    ci_low, ci_high = st.t.interval(0.95, len(scores) - 1, loc=mean_val, scale=st.sem(scores))
    # Restituisce l'intervallo di confidenza formattato
    return f"({round(ci_low, 6)}, {round(ci_high, 6)})"

print("Funzione confidence_interval ora disponibile.")

"""##### **Modello baseline**

Nel nostro progetto, il target è **binario**, rappresentato da due classi: "assenza di diabete" e "presenza di diabete".

Inizialmente, il dataset poteva essere sbilanciato, con un numero maggiore di pazienti senza diabete rispetto a quelli con diabete. Tuttavia, per bilanciare il dataset e trattare il problema della distribuzione sbilanciata delle classi, abbiamo applicato SMOTE (Synthetic Minority Over-sampling Technique), che ha generato esempi sintetici per la classe meno rappresentata, portando le classi a un numero simile di esempi.

Anche se il dataset è stato bilanciato tramite SMOTE, è comunque utile avere un modello baseline che predice la classe più comune nel caso di un dataset sbilanciato. Per questo motivo, il modello baseline predice sempre "presenza di diabete" (classe 1), senza prendere in considerazione le caratteristiche dei pazienti.

La logica del modello baseline è che, in un contesto di classi bilanciate, predire sempre la classe 1 può ancora dare un riferimento per l'accuratezza di altri modelli di machine learning. Infatti, lo scopo del modello baseline è fornire un **valore di accuratezza minima** da usare come **punto di riferimento** per confrontare altri modelli.

Se un modello di machine learning non riesce a superare l'accuratezza del modello baseline, ciò indicherebbe che il modello non ha apportato alcun valore aggiunto rispetto a un semplice modello che predice sempre la classe più frequente, con conseguente aumento dei costi computazionali senza miglioramenti effettivi.
"""

# Predizioni target sempre uguali a True (Presenza di Diabete)
y_pred_baseline = np.ones(len(y_test), dtype=bool)  # Predizione sempre 1 (Presenza)

# Matrice di confusione
cm = confusion_matrix(y_test, y_pred_baseline)

# Plottare la matrice di confusione
plt.figure(figsize=(6, 6))
sns.heatmap(cm, square=True, annot=True, fmt='d', cbar=True,
            xticklabels=['Sani', 'Diabetici'],
            yticklabels=['Sani', 'Diabetici'])
plt.ylabel('Reale')
plt.xlabel('Predetto')
plt.title('Matrice di Confusione - Modello Baseline')

# Mostrare il grafico
plt.show()

# Calcolare l'accuracy del modello baseline
baseline_test_accuracy = accuracy_score(y_test, y_pred_baseline)
print("Accuracy del modello baseline:", baseline_test_accuracy)

"""Interpretazione della Matrice di Confusione:
- Veri Positivi (TP):
  - 68 (predizione dei diabetici corretta per pazienti effettivamente diabetici)
- Falsi Positivi (FP):
  - 79 (predizione dei diabetici errata per pazienti effettivamente sani)
- Veri Negativi (TN):
  - 0 (non sono stati predetti sani per i pazienti effettivamente sani)
- Falsi Negativi (FN):
  - 0 (non sono stati predetti sani per i pazienti effettivamente diabetici)

Conclusione:
- Poiché il modello baseline predice sempre la classe "Diabetici" (classe 1), il modello ha fatto **68 previsioni corrette** (veri positivi, TP) per i pazienti diabetici. Tuttavia, ha **79 falsi positivi** (FP), ossia ha predetto che pazienti sani fossero diabetici.
- Il modello baseline **non distingue** affatto **i pazienti sani** (classe 0) e **predice solo i pazienti diabetici**. Questo porta a **un alto numero di falsi positivi**. La bassa accuratezza (46.26%) riflette che **il modello non è utile** in quanto **non riesce a fare predizioni corrette** per **la classe più rara** (ovvero i pazienti sani).

##### **Decision Tree Classifier**

Il Decision Tree Classifier è un modello di machine learning **supervisionato** utilizzato per la classificazione dei dati. Si basa su una **struttura ad albero** in cui ogni nodo rappresenta una **decisione basata su una caratteristica**.

###### **Addestramento dei modelli**

Per ottenere i risultati migliori, abbiamo ottimizzato il modello impostando i seguenti **iperparametri**:
- `criterion` (funzione di valutazione dell'albero),
- `splitter` (tipo di divisione per i nodi),
- `max_depth` (profondità massima dell'albero),
- `min_samples_split` (numero minimo di campioni per dividere un nodo),
- `ccp_alpha` (parametro di potatura per evitare overfitting).
"""

# Addestramento del modello sul training set (70/15/15)
dt_model = DecisionTreeClassifier(random_state=42, max_depth=15, min_samples_split=20, ccp_alpha=0.01, splitter='best', criterion='gini')
dt_model.fit(X_train, y_train)

"""Questa parte di codice esegue un'ottimizzazione dei parametri del modello DecisionTreeClassifier utilizzando **RandomizedSearchCV**, che cerca combinazioni casuali di parametri, tra cui:
- `dt_model` (il modello da ottimizzare).
- `n_iter=10` (il numero di combinazioni casuali da testare).
- `scoring='accuracy'` (la metrica di valutazione).
- `cv=10` (la validazione incrociata a 10 fold).
"""

# Definizione del grid dei parametri da esplorare
param_dist_dt = {
    'criterion': ['gini', 'entropy'],  # Funzione di valutazione
    'splitter': ['best', 'random'],  # Tipo di divisione
    'max_depth': [None, 5, 10, 15, 20],  # Profondità massima dell'albero
    'min_samples_split': [2, 5, 10, 20],  # Numero minimo di campioni per dividere un nodo
    'ccp_alpha': [0.0, 0.01, 0.1, 0.2],  # Parametro di potatura
}

# Creazione del RandomizedSearchCV per il modello (70/15/15)
random_search_dt_70 = RandomizedSearchCV(
    estimator=dt_model,
    param_distributions=param_dist_dt,
    n_iter=10,  # Numero di combinazioni casuali da testare
    scoring='accuracy',  # Puoi scegliere un altro parametro come precision, recall, etc.
    cv=10,  # 10-fold cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Addestramento del modello con RandomizedSearchCV sul training set (70/15/15)
random_search_dt_70.fit(X_train, y_train)

# Creazione del modello con i migliori parametri
best_dt_model_70 = random_search_dt_70.best_estimator_

# Mostra i migliori parametri trovati per ogni set
print("Migliori parametri per il modello (70/15/15):")
print(random_search_dt_70.best_params_)

"""###### **Valutazione con cross-validation (10-Fold)**

In questa parte di codice, viene eseguita una **10-Fold Cross-Validation** utilizzando il metodo **StratifiedKFold** per valutare il modello sui dati di addestramento.

Questo metodo suddivide i dati **in 10 parti** (fold), garantendo che ogni classe sia proporzionalmente rappresentata in ciascun fold, e aiuta a ottenere una stima più robusta delle performance del modello **su dati non visti**.
"""

# Creazione della cross-validation con StratifiedKFold (10 folds)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Esecuzione della cross-validation su training set (70/15/15)
cv_scores_70 = my_cross_validation(dt_model, X_train, y_train, cv=kfold)

# Creazione di un DataFrame per mostrare i risultati della cross-validation
cv_summary_df = pd.DataFrame({
    'Metrica': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Media (70/15/15)': [round(np.mean(cv_scores_70['accuracy']), 6),
                         round(np.mean(cv_scores_70['precision']), 6),
                         round(np.mean(cv_scores_70['recall']), 6),
                         round(np.mean(cv_scores_70['f1-score']), 6)],
    '95% CI (70/15/15)': [confidence_interval(cv_scores_70['accuracy']),
                           confidence_interval(cv_scores_70['precision']),
                           confidence_interval(cv_scores_70['recall']),
                           confidence_interval(cv_scores_70['f1-score'])]
})

cv_summary_df.set_index('Metrica', inplace=True)
cv_summary_df.index.name = None
cv_summary_df

"""Conclusioni:
- Il modello ha prestazioni elevate, con **metriche oltre l'80% in media**. Questo indica che il modello sta **generalizzando bene**, riuscendo a fare previsioni accurate sui dati di test senza sovradimensionare i dati di training.
- L'intervallo di confidenza relativamente stretto per tutte le metriche suggerisce una **performance stabile** su diverse suddivisioni dei dati.
- Il modello mostra un **buon equilibrio tra precisione e recall**, come evidenziato **dall'F1 score**.

###### **Controllo dell'overfitting**

Questo pezzo di codice esegue una **valutazione dell'overfitting** confrontando le prestazioni del modello Decision Tree sui set di training, validation e test.

Per il miglior controllo di overfitting, si calcola **la differenza tra Training e Test** per:
- Accuracy Gap > 0.1:
  - Il modello ha **overfitting**, perché la precisione sul training è molto più alta rispetto al test.

- F1 Score Gap > 0.1:
  - Il modello ha **difficoltà a generalizzare**, perché è molto più preciso sul training rispetto ai dati reali.

Quindi, se il valore fosse **vicino a zero o negativo**, significherebbe che il modello **generalizza bene** e **non soffre di overfitting**.

Invece, per identificare **l'underfitting**, si guardano **le percentuali delle metriche di performance** e(!) **il gap tra training e test**:
  - Se il modello mostra performance basse (ad esempio, sotto il 70%) su tutti i set e(!) il gap tra training e test è molto basso o negativo, è un segno che il modello non ha imparato abbastanza dai dati e non è sufficientemente complesso.
"""

# Predizioni sui diversi set per verificare l'overfitting (solo 70/15/15)
y_pred_train_dt = dt_model.predict(X_train)
y_pred_val_dt = dt_model.predict(X_val)
y_pred_test_dt = dt_model.predict(X_test)

# Creazione del DataFrame con le metriche per visualizzare l'overfitting
performance_df = pd.DataFrame({
    'Set': ['Train', 'Validation', 'Test'],
    'Accuracy (70/15/15)': [accuracy_score(y_train, y_pred_train_dt), accuracy_score(y_val, y_pred_val_dt), accuracy_score(y_test, y_pred_test_dt)],
    'Precision (70/15/15)': [precision_score(y_train, y_pred_train_dt, average='macro'), precision_score(y_val, y_pred_val_dt, average='macro'), precision_score(y_test, y_pred_test_dt, average='macro')],
    'Recall (70/15/15)': [recall_score(y_train, y_pred_train_dt, average='macro'), recall_score(y_val, y_pred_val_dt, average='macro'), recall_score(y_test, y_pred_test_dt, average='macro')],
    'F1 Score (70/15/15)': [f1_score(y_train, y_pred_train_dt, average='macro'), f1_score(y_val, y_pred_val_dt, average='macro'), f1_score(y_test, y_pred_test_dt, average='macro')]
})

# Calcolo della differenza tra Train e Test per ogni metrica
performance_df["Accuracy Gap (Train - Test)"] = performance_df["Accuracy (70/15/15)"][0] - performance_df["Accuracy (70/15/15)"][2]
performance_df["F1 Score Gap (Train - Test)"] = performance_df["F1 Score (70/15/15)"][0] - performance_df["F1 Score (70/15/15)"][2]


# Visualizza il risultato
print("Decision Tree Performance Overfitting Check with Gaps")
performance_df.set_index('Set', inplace=True)
performance_df.index.name = None
performance_df

"""Conclusioni:
- **Le differenze** tra training, validation e test sono **minime**, il che è un **segno positivo**. Un modello che generalizza bene tende a non avere grandi differenze tra queste metriche, mentre un modello che soffre di overfitting avrebbe una prestazione significativamente migliore sui dati di training rispetto ai dati di test.
- Accuracy Gap (Train - Test) e F1 Score Gap (Train - Test) sono **entrambi negativi** (circa -0.014), il che indica che la performance del modello sui dati di test è leggermente migliore di quella sui dati di training e validation. Questo potrebbe suggerire che il modello **non è affatto sovradimensionato** e si sta **generalizzando molto bene**.

###### **Alberi, matrici di correlazione e grafici**
"""

plt.figure(figsize=(30, 10))
plot_tree(dt_model, filled=True, rounded=True, class_names=['Sani', 'Diabetici'], feature_names=y)
plt.show()

"""Conclusioni:
- L'albero decisionale sta cercando di classificare i dati in due classi principali: Sani e Diabetici.
- L'algoritmo ha costruito vari nodi basandosi sulle caratteristiche del dataset, tentando di minimizzare l'impurità Gini in ciascun ramo per ottenere una classificazione più accurata.
- L'albero sembra ben strutturato, con valori Gini relativamente bassi, il che indica che le suddivisioni fatte per ogni nodo sono efficaci nel separare le classi.

In sintesi, questa è una visualizzazione di come un albero decisionale ha suddiviso il dataset per classificare le persone in due categorie, cercando di ottimizzare la separazione delle classi (Sani e Diabetici) in base alle caratteristiche del dataset.
"""

# Definizione delle etichette delle classi
class_labels = ["Sani", "Diabetici"]

# Creazione della figura per le confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Solo 3 grafici ora

# Titoli per i grafici (solo 70/15/15)
titles = [
    "Train Set (70/15/15)", "Validation Set (70/15/15)", "Test Set (70/15/15)"
]

# Liste delle confusion matrix per ogni set di dati (solo 70/15/15)
cm_data = [
    confusion_matrix(y_train, y_pred_train_dt),
    confusion_matrix(y_val, y_pred_val_dt),
    confusion_matrix(y_test, y_pred_test_dt)
]

# Loop per creare ogni subplot con la rispettiva matrice di confusione
for i, ax in enumerate(axes.flatten()):
    sns.heatmap(cm_data[i], annot=True, fmt='d', cmap='coolwarm', ax=ax,
                xticklabels=class_labels, yticklabels=class_labels, cbar=False)
    ax.set_title(titles[i], fontsize=14, fontweight="bold")
    ax.set_xlabel("Predetto", fontsize=12)
    ax.set_ylabel("Reale", fontsize=12)

# Miglioramento della disposizione dei grafici
plt.tight_layout()
plt.show()

"""Conclusioni:
- **Train Set** ha una **buona** capacità di **distinguere le classi** nel set di addestramento.
- **Validation Set** sta generalizzando abbastanza bene anche **sui dati di validazione**, senza evidenti segni di overfitting.
- **Test Set** indica che la performance è simile a quella nel set di validazione, indicando che il modello è riuscito a mantenere una buona precisione anche **sui dati mai visti prima**.

"""

# Risultati della Cross-Validation (solo per 70/15/15)
accuracy_dt = round(np.mean(cv_scores_70['accuracy']), 6)
precision_dt = round(np.mean(cv_scores_70['precision']), 6)
recall_dt = round(np.mean(cv_scores_70['recall']), 6)
f1_dt = round(np.mean(cv_scores_70['f1-score']), 6)

# DataFrame con le Metriche di Performance (solo per 70/15/15)
performance_dt_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Decision Tree (70/15/15)': [accuracy_dt * 100, precision_dt * 100, recall_dt * 100, f1_dt * 100]
}).set_index('Metric').T

# Curva ROC (solo per 70/15/15)
fpr_dt, tpr_dt, _ = roc_curve(y_test, dt_model.predict_proba(X_test)[:, 1])  # ROC per Decision Tree (70/15/15)
roc_auc_dt = auc(fpr_dt, tpr_dt)

# Creazione della figura
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

### Grafico delle metriche di performance
performance_dt_df.plot(kind='bar', ax=axes[0], colormap='viridis')
axes[0].set_title("Confronto delle Metriche di Performance per Decision Tree", fontsize=14)
axes[0].set_ylabel("Score (%)", fontsize=12)
axes[0].set_ylim(0, 100)  # Scala 0-100%
axes[0].legend(title="Metriche", loc="lower right", fontsize=10)
axes[0].tick_params(axis='x', labelrotation=0)

# Annotazioni sopra le barre
for container in axes[0].containers:
    axes[0].bar_label(container, fmt='%.2f%%', fontsize=10)

### Grafico della Curva ROC (solo per 70/15/15)
axes[1].plot(fpr_dt, tpr_dt, color='red', lw=2, label=f'DT AUC = {roc_auc_dt:.4f}')

# Linea di riferimento per il caso casuale
axes[1].plot([0, 1], [0, 1], color='grey', linestyle='--')

axes[1].set_xlabel("False Positive Rate", fontsize=12)
axes[1].set_ylabel("True Positive Rate", fontsize=12)
axes[1].set_title("Curva ROC per Decision Tree", fontsize=14)
axes[1].legend(loc="lower right", fontsize=10)

# Miglioramento del layout
plt.tight_layout()
plt.show()

"""Conclusioni dalle Metriche di Performance:
- Accuracy: 84.25%
  - Il modello ha una buona accuratezza, indicando che il 84.25% delle predizioni sono corrette. Questa è una metrica generale che rappresenta la percentuale di predizioni giuste.

- Precision: 84.69%
  - La precisione è molto simile all'accuracy e indica che quando il modello predice la classe "Sani" o "Diabetici", la previsione è corretta nel 84.69% dei casi. Questo è un buon risultato, che indica che il modello è abbastanza preciso nell'evitare falsi positivi.

- Recall: 84.25%
  - Il recall misura quanto bene il modello riesce a rilevare tutte le istanze delle classi "Sani" e "Diabetici". Il valore di 84.25% è molto simile a quello dell'accuracy, suggerendo che il modello è equilibrato nel rilevare entrambe le classi senza tralasciarne troppe.

- F1 Score: 84.18%
  - L'F1 score è una misura combinata che tiene conto sia della precisione che del recall. Un valore di 84.18% indica che il modello ha un buon equilibrio tra precisione e recall, senza favorire una classe rispetto all'altra.


Conclusioni:
- Il modello Decision Tree ha ottime performance con una precisione e recall simili (entrambi intorno all'84%), indicando che non favorisce né la classe "Sani" né la classe "Diabetici" in modo sbagliato.
- La curva ROC e l'AUC indicano che il modello è altamente competente nel separare le classi, facendo pochissimi errori.
- Nel complesso, possiamo dire che il Decision Tree è un modello solido per la classificazione tra Sani e Diabetici.

##### **Random Forest Classifier**

Il Random Forest Classifier è un modello di machine learning **supervisionato** utilizzato per la classificazione dei dati. Si basa su **un insieme di alberi decisionali** (decision trees) che lavorano in modo collaborativo per migliorare le prestazioni complessive del modello.

**Ogni albero** nella foresta è costruito su **un sottoinsieme casuale dei dati e delle caratteristiche** (tecnica di bagging). La predizione finale viene determinata attraverso un processo di **voto di maggioranza tra gli alberi**, il che rende il modello **robusto contro l’overfitting** e adatto per gestire dataset complessi e sbilanciati.

###### **Addestramento dei modelli**

Per ottenere i risultati migliori, abbiamo ottimizzato il modello impostando i seguenti **iperparametri**:
- `n_estimators` (numero di alberi nel Random Forest)
- `max_depth` (la profondità massima dell'albero, che limita la crescita dell'albero per evitare l'overfitting)
- `min_samples_split` (numero minimo di campioni richiesti per dividere un nodo)
- `min_samples_leaf` (numero minimo di campioni per essere una foglia)
- `max_features` (percentuale del numero di caratteristiche da considerare per ciascuna divisione)
- `bootstrap` (utilizza il campionamento bootstrap per migliorare l'affidabilità del modello)
- `oob_score` (out-of-bag score, utilizzato per la valutazione del modello durante l'addestramento)
- `n_jobs` (numero di CPU da usare per il calcolo parallelo, impostato su -1 per utilizzare tutte le CPU disponibili)
- `class_weight` (pesatura delle classi)
- `criterion` (funzione di valutazione dei nodi dell'albero)
"""

# Addestramento del modello sul training set (70/15/15)
rf_model = RandomForestClassifier(
    random_state=42,
    n_estimators=200,
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=2,
    max_features=0.8,
    bootstrap=True,
    oob_score=True,
    n_jobs=-1,
    class_weight='balanced',
    criterion='entropy'
)
rf_model.fit(X_train, y_train)

"""Questa parte di codice esegue un'ottimizzazione dei parametri del modello RandomForestClassifier utilizzando RandomizedSearchCV, che esplora combinazioni casuali di parametri, tra cui:
  - `rf_model` (il modello da ottimizzare).
  - `n_iter=10` (il numero di combinazioni casuali da testare).
  - `scoring='accuracy'` (la metrica di valutazione, che puoi cambiare in precision, recall, ecc.).
  - `cv=10` (la validazione incrociata a 10 fold).
"""

# Definizione dei parametri con controllo su bootstrap e oob_score
param_dist = {
    'n_estimators': [50, 100, 200, 300],  # Numero di alberi nel Random Forest
    'max_depth': [5, 10, 15, None],  # Profondità massima dell'albero
    'min_samples_split': [2, 5, 10, 20],  # Numero minimo di campioni per dividere un nodo
    'min_samples_leaf': [1, 2, 4],  # Numero minimo di campioni per una foglia
    'max_features': ['sqrt', 'log2', 0.5, 0.8],  # Percentuale di caratteristiche da considerare per ogni split
    'bootstrap': [True, False],  # Se abilitato, utilizza il campionamento bootstrap
    'oob_score': [True, False],  # Può essere True solo quando bootstrap è True
    'n_jobs': [-1],  # Usa tutti i core disponibili per il calcolo parallelo
    'random_state': [42],  # Fisso per ripetibilità dei risultati
    'class_weight': [None, 'balanced'],  # Gestione delle classi sbilanciate
    'criterion': ['gini', 'entropy']  # Criterio di valutazione per la divisione
}


# Correzione: Impostiamo che oob_score è True solo quando bootstrap è True
param_dist = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', 0.5, 0.8],
    'bootstrap': [True],  # Solo quando bootstrap=True
    'oob_score': [True],  # oob_score=True solo quando bootstrap=True
    'n_jobs': [-1],  # Usa tutti i core disponibili
    'random_state': [42],
    'class_weight': [None, 'balanced'],
    'criterion': ['gini', 'entropy']
}

# Creazione del RandomizedSearchCV
random_search_70 = RandomizedSearchCV(
    estimator=rf_model,  # Il modello Random Forest da ottimizzare
    param_distributions=param_dist,  # Grid dei parametri da esplorare
    n_iter=10,  # Numero di combinazioni casuali da testare
    scoring='accuracy',  # Metrica di valutazione
    cv=10,  # Validazione incrociata a 10 fold
    verbose=1,  # Dettagli durante l'esecuzione
    random_state=42,  # Per garantire la ripetibilità dei risultati
    n_jobs=-1  # Usa tutti i core disponibili per il calcolo parallelo
)

# Addestramento del modello con RandomizedSearchCV sul training set (70/15/15)
random_search_70.fit(X_train, y_train)

# Creazione del modello con i migliori parametri
best_rf_model_70 = random_search_70.best_estimator_

# Mostra i migliori parametri trovati
print("Migliori parametri per il modello (70/15/15):")
print(random_search_70.best_params_)

"""###### **Valutazione con cross-validation (10-Fold)**

In questa parte di codice, viene eseguita una **10-Fold Cross-Validation** utilizzando il metodo **StratifiedKFold** per valutare il modello sui dati di addestramento.

Questo metodo suddivide i dati **in 10 parti** (fold), garantendo che ogni classe sia proporzionalmente rappresentata in ciascun fold, e aiuta a ottenere una stima più robusta delle performance del modello **su dati non visti**.
"""

# Creazione della cross-validation con StratifiedKFold (10 folds)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Esecuzione della cross-validation su training set (70/15/15)
cv_scores_rf_70 = my_cross_validation(rf_model, X_train, y_train, cv=kfold)

# Creazione di un DataFrame per mostrare i risultati della cross-validation
cv_summary_rf_df = pd.DataFrame({
    'Metrica': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Media (70/15/15)': [round(np.mean(cv_scores_rf_70['accuracy']), 6),
                         round(np.mean(cv_scores_rf_70['precision']), 6),
                         round(np.mean(cv_scores_rf_70['recall']), 6),
                         round(np.mean(cv_scores_rf_70['f1-score']), 6)],
    '95% CI (70/15/15)': [confidence_interval(cv_scores_rf_70['accuracy']),
                           confidence_interval(cv_scores_rf_70['precision']),
                           confidence_interval(cv_scores_rf_70['recall']),
                           confidence_interval(cv_scores_rf_70['f1-score'])],
})

cv_summary_rf_df.set_index('Metrica', inplace=True)
cv_summary_rf_df.index.name = None
cv_summary_rf_df

"""Conclusioni:
- Il modello Random Forest ha ottenuto valori superiori all'80% per tutte le metriche, con una media dell'accuracy di 88.19%. Questo indica che il modello sta facendo un buon lavoro nel fare previsioni accurate sui dati di test senza sovradimensionare i dati di training.
- L'intervallo di confidenza relativamente stretto per tutte le metriche (con intervalli tra 0.858 e 0.905) suggerisce che il modello ha prestazioni stabili su diverse suddivisioni dei dati. Questo è un segno positivo di generalizzazione, indicando che il modello è robusto.
-  Il modello ha ottenuto un F1 Score di 88.15%, che evidenzia un buon equilibrio tra precisione e recall. Questo significa che il modello è capace di rilevare correttamente sia le istanze di "Sani" che quelle di "Diabetici", riducendo sia i falsi positivi che i falsi negativi.


Da notare che **rispetto al modello Decision Tree**, **il modello Random Forest** ha mostrato **prestazioni superiori in tutte le metriche**.

Questo suggerisce che Random Forest **offre un modello più potente e robusto** per questa classificazione, probabilmente grazie alla sua natura di **ensemble learning**, che **riduce la varianza** e **migliora la stabilità rispetto ai singoli alberi decisionali**.

###### **Controllo dell'overfitting**

Questo pezzo di codice esegue una **valutazione dell'overfitting** confrontando le prestazioni del modello Decision Tree sui set di training, validation e test.

Per il miglior controllo di overfitting, si calcola **la differenza tra Training e Test** per:
- Accuracy Gap > 0.1:
  - Il modello ha **overfitting**, perché la precisione sul training è molto più alta rispetto al test.

- F1 Score Gap > 0.1:
  - Il modello ha **difficoltà a generalizzare**, perché è molto più preciso sul training rispetto ai dati reali.

Quindi, se il valore fosse **vicino a zero o negativo**, significherebbe che il modello **generalizza bene** e **non soffre di overfitting**.

Invece, per identificare **l'underfitting**, si guardano **le percentuali delle metriche di performance** e(!) **il gap tra training e test**:
  - Se il modello mostra performance basse (ad esempio, sotto il 70%) su tutti i set e(!) il gap tra training e test è molto basso o negativo, è un segno che il modello non ha imparato abbastanza dai dati e non è sufficientemente complesso.
"""

# Predizioni sui diversi set per verificare l'overfitting con Random Forest
y_pred_train_rf = rf_model.predict(X_train)
y_pred_val_rf = rf_model.predict(X_val)
y_pred_test_rf = rf_model.predict(X_test)

# Creazione del DataFrame con le metriche per visualizzare l'overfitting (solo per 70/15/15)
performance_rf_df = pd.DataFrame({
    'Set': ['Train', 'Validation', 'Test'],
    'Accuracy (70/15/15)': [accuracy_score(y_train, y_pred_train_rf), accuracy_score(y_val, y_pred_val_rf), accuracy_score(y_test, y_pred_test_rf)],
    'Precision (70/15/15)': [precision_score(y_train, y_pred_train_rf, average='macro'), precision_score(y_val, y_pred_val_rf, average='macro'), precision_score(y_test, y_pred_test_rf, average='macro')],
    'Recall (70/15/15)': [recall_score(y_train, y_pred_train_rf, average='macro'), recall_score(y_val, y_pred_val_rf, average='macro'), recall_score(y_test, y_pred_test_rf, average='macro')],
    'F1 Score (70/15/15)': [f1_score(y_train, y_pred_train_rf, average='macro'), f1_score(y_val, y_pred_val_rf, average='macro'), f1_score(y_test, y_pred_test_rf, average='macro')]
})

# Calcolo della differenza tra Train e Test per ogni metrica
performance_rf_df["Accuracy Gap (Train - Test)"] = performance_rf_df["Accuracy (70/15/15)"][0] - performance_rf_df["Accuracy (70/15/15)"][2]
performance_rf_df["F1 Score Gap (Train - Test)"] = performance_rf_df["F1 Score (70/15/15)"][0] - performance_rf_df["F1 Score (70/15/15)"][2]

# Mostra le performance e l'overfitting
print("Random Forest Performance Overfitting Check with Gaps")
performance_rf_df.set_index('Set', inplace=True)
performance_rf_df.index.name = None
performance_rf_df

"""Conclusioni:
  - Il gap tra l'accuratezza del training e quella del test è 0.081 per entrambe le metriche (Accuracy e F1 score). Questo valore suggerisce che il modello **non è completamente sovradimensionato** (overfitting), poiché il gap tra i dati di addestramento e quelli di test **è relativamente stabile**, e **non c'è una grande differenza di performance**.
  - Tuttavia, il gap esiste ancora, il che indica che il modello **potrebbe aver appreso troppo bene** le caratteristiche del training set, ma è comunque in grado di generalizzare bene.
  - Il valore di F1 score per training, validation e test è molto simile (0.972 per il training e 0.925 per validation/test), suggerendo che il modello mantiene **un buon equilibrio tra precisione e recall** su tutti i set, senza favorire troppo una classe rispetto all'altra.

Dunque, il modello Random Forest mostra prestazioni eccellenti in tutte le metriche (accuracy, precision, recall, F1 score) con valori superiori all'80% e un buon equilibrio tra precisione e recall. Il gap tra il set di training e il set di test è minimo, indicando una buona generalizzazione. Tuttavia, un piccolo gap suggerisce che potrebbe esserci ancora una certa tendenza al leggero overfitting. Complessivamente, il modello è molto robusto e ben adatto alla classificazione.

###### **Matrici di correlazione e grafici**
"""

# Definizione delle etichette delle classi
class_labels = ["Sani", "Diabetici"]

# Creazione della figura per le confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Solo 3 grafici ora

# Titoli per i grafici (solo 70/15/15)
titles = [
    "Train Set (70/15/15)", "Validation Set (70/15/15)", "Test Set (70/15/15)"
]

# Liste delle confusion matrix per ogni set di dati con Random Forest (solo 70/15/15)
cm_rf_data = [
    confusion_matrix(y_train, y_pred_train_rf),
    confusion_matrix(y_val, y_pred_val_rf),
    confusion_matrix(y_test, y_pred_test_rf)
]

# Loop per creare ogni subplot con la rispettiva matrice di confusione
for i, ax in enumerate(axes.flatten()):
    sns.heatmap(cm_rf_data[i], annot=True, fmt='d', cmap='coolwarm', ax=ax,
                xticklabels=class_labels, yticklabels=class_labels, cbar=False)
    ax.set_title(titles[i], fontsize=14, fontweight="bold")
    ax.set_xlabel("Predetto", fontsize=12)
    ax.set_ylabel("Reale", fontsize=12)

# Miglioramento della disposizione dei grafici
plt.tight_layout()
plt.show()

"""Conclusioni:
  - Il modello Random Forest ha ottime prestazioni in tutti i set di dati, con una bassa incidenza di errori e una buona capacità di generalizzazione, mostrando risultati coerenti su training, validation e test. Non ci sono evidenti segnali di overfitting, con performance solide anche sui dati non visti.
"""

# Risultati della Cross-Validation per Random Forest (solo per 70/15/15)
accuracy_rf = round(np.mean(cv_scores_rf_70['accuracy']), 6)
precision_rf = round(np.mean(cv_scores_rf_70['precision']), 6)
recall_rf = round(np.mean(cv_scores_rf_70['recall']), 6)
f1_rf = round(np.mean(cv_scores_rf_70['f1-score']), 6)

# DataFrame con le Metriche di Performance per Random Forest (solo per 70/15/15)
performance_rf_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Random Forest (70/15/15)': [accuracy_rf * 100, precision_rf * 100, recall_rf * 100, f1_rf * 100]
}).set_index('Metric').T

# Curva ROC per Random Forest (solo per 70/15/15)
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_model.predict_proba(X_test)[:, 1])  # ROC per Random Forest (70/15/15)
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Creazione della figura
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

### Grafico delle metriche di performance per Random Forest
performance_rf_df.plot(kind='bar', ax=axes[0], colormap='viridis')
axes[0].set_title("Confronto delle Metriche di Performance per Random Forest", fontsize=14)
axes[0].set_ylabel("Score (%)", fontsize=12)
axes[0].set_ylim(0, 100)  # Scala 0-100%
axes[0].legend(title="Metriche", loc="lower right", fontsize=10)
axes[0].tick_params(axis='x', labelrotation=0)

# Annotazioni sopra le barre con i valori percentuali
for container in axes[0].containers:
    axes[0].bar_label(container, fmt='%.2f%%', fontsize=10)

### Grafico della Curva ROC per Random Forest
axes[1].plot(fpr_rf, tpr_rf, color='red', lw=2, label=f'RF AUC = {roc_auc_rf:.4f}')

# Linea di riferimento per il caso casuale
axes[1].plot([0, 1], [0, 1], color='grey', linestyle='--')

axes[1].set_xlabel("False Positive Rate", fontsize=12)
axes[1].set_ylabel("True Positive Rate", fontsize=12)
axes[1].set_title("Curva ROC per Random Forest", fontsize=14)
axes[1].legend(loc="lower right", fontsize=10)

# Miglioramento del layout
plt.tight_layout()
plt.show()

"""Conclusioni:
- Il modello ha ottenuto prestazioni molto elevate in tutte le metriche, con valori di accuracy, precision, recall, e F1 score attorno all'88%. Questo indica che il modello è equilibrato e riesce a classificare correttamente sia le classi "Sani" che "Diabetici", con prestazioni molto simili su tutte le metriche, suggerendo una buona generalizzazione.
- La curva ROC (Receiver Operating Characteristic) mostra un comportamento eccellente con un AUC di 0.9657, che indica una ottima capacità del modello di discriminare tra le classi. Un AUC vicino a 1 suggerisce che il modello è in grado di classificare correttamente le istanze in modo affidabile.

##### **Support Vector Machine Classifier**

Il Support Vector Machine (SVM) è un modello di machine learning **supervisionato** utilizzato per la classificazione dei dati. Si basa sul concetto di **iperpiano ottimale**, che **separa le classi massimizzando il margine tra i punti dati più vicini** (support vectors).

L'SVM può essere utilizzato con diverse **funzioni di kernel** per adattarsi a distribuzioni di **dati non lineari**:

- Linear Kernel: utile per dati separabili linearmente.

- Polynomial Kernel: applica una trasformazione polinomiale ai dati.

- RBF (Radial Basis Function) Kernel: il più comune, adatto a problemi complessi con separabilità non lineare.

- Sigmoid Kernel: utilizzato in scenari specifici, come il riconoscimento di pattern.

L'SVM è particolarmente efficace nei problemi di **classificazione binaria** ed è noto per la sua **robustezza rispetto agli outlier** e alla **capacità di generalizzazione**.

###### **Addestramento dei modelli**

Per ottenere i risultati migliori, abbiamo ottimizzato il modello impostando i seguenti iperparametri:

- `C` (parametro di regolarizzazione, controlla quanto il modello penalizza gli errori)
- `kernel` (tipo di kernel da utilizzare per il calcolo del margine)
- `gamma` (parametro che determina la forma della funzione del kernel, come la "larghezza" del margine)
- `probability` (abilita la probabilità per la curva ROC)

Da notare che il modello SVM con **kernel RBF** è stato scelto perché offre una buona capacità di generalizzazione anche in scenari con dati non linearmente separabili.
"""

# Addestramento del modello sul training set (70/15/15)
svm_model = SVC(
    kernel='rbf',                       # Kernel Radial Basis Function (RBF)
    C=1.4391207615728072,               # Parametro di regolarizzazione
    gamma='scale',                      # Parametro gamma per il kernel
    probability=True,                   # Permette di ottenere probabilità per la curva ROC
    random_state=42
)

svm_model.fit(X_train, y_train)

"""Questa parte di codice esegue un'ottimizzazione dei parametri del modello SVC utilizzando RandomizedSearchCV, che esplora combinazioni casuali di parametri, tra cui:
  - `svm_model` (il modello da ottimizzare).
  - `n_iter=50` (il numero di combinazioni casuali da testare).
  - `scoring='accuracy'` (la metrica di valutazione, che puoi cambiare in precision, recall, ecc.).
  - `cv=10` (la validazione incrociata a 10 fold).
"""

# Parametri da esplorare
param_dist = {
    'C': loguniform(1e-6, 1e3),
    'gamma': ['scale'],
    'kernel': ['rbf'],
}

# Esegui RandomizedSearchCV con i parametri definiti
random_search_svm = RandomizedSearchCV(
    estimator=svm_model,
    param_distributions=param_dist,
    n_iter=50,    # Numero di iterazioni
    cv=10,        # 10-fold cross-validation
    n_jobs=-1,    # Usa tutti i core
    verbose=2,    # Mostra il progresso
    random_state=42,
    scoring='accuracy'
)

# Esegui la ricerca
random_search_svm.fit(X_train, y_train)

# Risultati migliori
print("Migliori parametri trovati:", random_search_svm.best_params_)

# Accedi al miglior modello
best_model_svm = random_search_svm.best_estimator_

# Valutazione finale sul test set
score_svm = best_model_svm.score(X_test, y_test)
print(f"Accuratezza del miglior modello (random search): {score_svm}")

"""###### **Valutazione con cross-validation (10-Fold)**

In questa parte di codice, viene eseguita una **10-Fold Cross-Validation** utilizzando il metodo **StratifiedKFold** per valutare il modello sui dati di addestramento.

Questo metodo suddivide i dati **in 10 parti** (fold), garantendo che ogni classe sia proporzionalmente rappresentata in ciascun fold, e aiuta a ottenere una stima più robusta delle performance del modello **su dati non visti**.
"""

# Creazione della cross-validation con StratifiedKFold (10 folds)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Esecuzione della cross-validation su training set (70/15/15)
cv_scores_svm_70 = my_cross_validation(svm_model, X_train, y_train, cv=kfold)

# Creazione di un DataFrame per mostrare i risultati della cross-validation
cv_summary_svm_df = pd.DataFrame({
    'Metrica': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Media (70/15/15)': [round(np.mean(cv_scores_svm_70['accuracy']), 6),
                         round(np.mean(cv_scores_svm_70['precision']), 6),
                         round(np.mean(cv_scores_svm_70['recall']), 6),
                         round(np.mean(cv_scores_svm_70['f1-score']), 6)],
    '95% CI (70/15/15)': [confidence_interval(cv_scores_svm_70['accuracy']),
                           confidence_interval(cv_scores_svm_70['precision']),
                           confidence_interval(cv_scores_svm_70['recall']),
                           confidence_interval(cv_scores_svm_70['f1-score'])],
})

cv_summary_svm_df.set_index('Metrica', inplace=True)
cv_summary_svm_df.index.name = None
cv_summary_svm_df

"""Conclusioni:
  - Le metriche di accuracy (84.40%), precision (84.73%), recall (84.42%), e F1 score (84.36%) sono tutte molto alte, indicando che il modello è in grado di fare previsioni accurate, sia per la classe "Sani" che per la classe "Diabetici".
  - L'intervallo di confidenza (CI) per ciascuna metrica è relativamente stretto, il che suggerisce che il modello ha prestazioni stabili attraverso i vari fold di validazione.
  - Le metriche di precision e recall sono molto simili, il che indica un buon equilibrio tra l'identificazione corretta delle classi e la riduzione dei falsi positivi e negativi. L'F1 score, che bilancia precisione e recall, è anch'esso molto alto, suggerendo che il modello è ben bilanciato e non favorisce una classe rispetto all'altra.
  - La performance di accuracy e F1 score che supera l'84% suggerisce che il modello è in grado di generalizzare bene sui dati di test non visti, il che è un segno positivo in termini di prevenzione dell'overfitting.

###### **Controllo dell'overfitting**

Questo pezzo di codice esegue una **valutazione dell'overfitting** confrontando le prestazioni del modello SVM sui set di training, validation e test.

Per il miglior controllo di overfitting, si calcola **la differenza tra Training e Test** per:
- Accuracy Gap > 0.1:
  - Il modello ha **overfitting**, perché la precisione sul training è molto più alta rispetto al test.

- F1 Score Gap > 0.1:
  - Il modello ha **difficoltà a generalizzare**, perché è molto più preciso sul training rispetto ai dati reali.

Quindi, se il valore fosse **vicino a zero o negativo**, significherebbe che il modello **generalizza bene** e **non soffre di overfitting**.

Invece, per identificare **l'underfitting**, si guardano **le percentuali delle metriche di performance** e(!) **il gap tra training e test**:
  - Se il modello mostra performance basse (ad esempio, sotto il 70%) su tutti i set e(!) il gap tra training e test è molto basso o negativo, è un segno che il modello non ha imparato abbastanza dai dati e non è sufficientemente complesso.
"""

# Predizioni sui diversi set per verificare l'overfitting con SVM (solo 70/15/15)
y_pred_train_svm = svm_model.predict(X_train)
y_pred_val_svm = svm_model.predict(X_val)
y_pred_test_svm = svm_model.predict(X_test)

# Creazione del DataFrame con le metriche per visualizzare l'overfitting (solo per 70/15/15)
performance_svm_df = pd.DataFrame({
    'Set': ['Train', 'Validation', 'Test'],
    'Accuracy (70/15/15)': [accuracy_score(y_train, y_pred_train_svm), accuracy_score(y_val, y_pred_val_svm), accuracy_score(y_test, y_pred_test_svm)],
    'Precision (70/15/15)': [precision_score(y_train, y_pred_train_svm, average='macro'), precision_score(y_val, y_pred_val_svm, average='macro'), precision_score(y_test, y_pred_test_svm, average='macro')],
    'Recall (70/15/15)': [recall_score(y_train, y_pred_train_svm, average='macro'), recall_score(y_val, y_pred_val_svm, average='macro'), recall_score(y_test, y_pred_test_svm, average='macro')],
    'F1 Score (70/15/15)': [f1_score(y_train, y_pred_train_svm, average='macro'), f1_score(y_val, y_pred_val_svm, average='macro'), f1_score(y_test, y_pred_test_svm, average='macro')]
})

# Calcolo della differenza tra Train e Test per ogni metrica
performance_svm_df["Accuracy Gap (Train - Test)"] = performance_svm_df["Accuracy (70/15/15)"][0] - performance_svm_df["Accuracy (70/15/15)"][2]
performance_svm_df["F1 Score Gap (Train - Test)"] = performance_svm_df["F1 Score (70/15/15)"][0] - performance_svm_df["F1 Score (70/15/15)"][2]

# Mostra le performance e l'overfitting
print("SVM Performance Overfitting Check with Gaps")
performance_svm_df.set_index('Set', inplace=True)
performance_svm_df.index.name = None
performance_svm_df

"""Conclusioni:
  - L'accuratezza e le altre metriche di performance (precisione, recall e F1 score) sui set di training e validation sono molto elevate, con valori superiori all'90%.
  - La precisione e recall sono anche molto simili sui set di training e validazione, indicando che il modello sta classificando correttamente entrambe le classi senza favorire una rispetto all'altra.
  - Il gap tra il set di training e il test (circa 1.7% per l'accuratezza e l'F1 score) è relativamente piccolo, il che suggerisce che il modello **non sta soffrendo di overfitting**.
  - Un gap simile tra training e validation implica che il modello **è in grado di generalizzare bene** anche sui dati di validation, senza sovradimensionare troppo i dati di training.

###### **Matrici di correlazione e grafici**
"""

# Definizione delle etichette delle classi
class_labels = ["Sani", "Diabetici"]

# Creazione della figura per le confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Titoli per i grafici
titles = [
    "Train Set (70/15/15)", "Validation Set (70/15/15)", "Test Set (70/15/15)"
]

# Liste delle confusion matrix per ogni set di dati con SVM (solo 70/15/15)
cm_svm_data = [
    confusion_matrix(y_train, y_pred_train_svm),
    confusion_matrix(y_val, y_pred_val_svm),
    confusion_matrix(y_test, y_pred_test_svm)
]

# Loop per creare ogni subplot con la rispettiva matrice di confusione
for i, ax in enumerate(axes.flatten()):
    sns.heatmap(cm_svm_data[i], annot=True, fmt='d', cmap='coolwarm', ax=ax,
                xticklabels=class_labels, yticklabels=class_labels, cbar=False)
    ax.set_title(titles[i], fontsize=14, fontweight="bold")
    ax.set_xlabel("Predetto", fontsize=12)
    ax.set_ylabel("Reale", fontsize=12)

# Miglioramento della disposizione dei grafici
plt.tight_layout()
plt.show()

"""Conclusioni:
  - Sebbene il modello continui a performare bene sui dati di test, c'è una leggera diminuzione nei falsi negativi (71 per "Sani") e nei falsi positivi (59 per "Diabetici"). Questo è un comportamento comune in qualsiasi modello che si applica a nuovi dati, ma le metriche rimangono elevate, indicanti una buona capacità di generalizzazione.
"""

# Risultati della Cross-Validation per SVM (solo 70/15/15)
accuracy_svm = round(np.mean(cv_scores_svm_70['accuracy']), 6)
precision_svm = round(np.mean(cv_scores_svm_70['precision']), 6)
recall_svm = round(np.mean(cv_scores_svm_70['recall']), 6)
f1_svm = round(np.mean(cv_scores_svm_70['f1-score']), 6)

# DataFrame con le Metriche di Performance per SVM (solo 70/15/15)
performance_svm_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'SVM (70/15/15)': [accuracy_svm * 100, precision_svm * 100, recall_svm * 100, f1_svm * 100]
}).set_index('Metric').T

# Curva ROC per SVM (70/15/15)
fpr_svm, tpr_svm, _ = roc_curve(y_test, svm_model.decision_function(X_test))  # ROC per SVM
roc_auc_svm = auc(fpr_svm, tpr_svm)

# Creazione della figura
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

### Grafico delle metriche di performance per SVM
performance_svm_df.plot(kind='bar', ax=axes[0], colormap='viridis')
axes[0].set_title("Confronto delle Metriche di Performance per SVM", fontsize=14)
axes[0].set_ylabel("Score (%)", fontsize=12)
axes[0].set_ylim(0, 100)  # Scala 0-100%
axes[0].legend(title="Metriche", loc="lower right", fontsize=10)
axes[0].tick_params(axis='x', labelrotation=0)

# Annotazioni sopra le barre con i valori percentuali
for container in axes[0].containers:
    axes[0].bar_label(container, fmt='%.2f%%', fontsize=10)

### Grafico della Curva ROC per SVM
axes[1].plot(fpr_svm, tpr_svm, color='red', lw=2, label=f'SVM AUC = {roc_auc_svm:.4f}')

# Linea di riferimento per il caso casuale
axes[1].plot([0, 1], [0, 1], color='grey', linestyle='--')

axes[1].set_xlabel("False Positive Rate", fontsize=12)
axes[1].set_ylabel("True Positive Rate", fontsize=12)
axes[1].set_title("Curva ROC per SVM", fontsize=14)
axes[1].legend(loc="lower right", fontsize=10)

# Miglioramento del layout
plt.tight_layout()
plt.show()

"""Conclusioni:
  - La curva ROC mostra un buon tasso di True Positive Rate (TPR) rispetto al False Positive Rate (FPR), con un AUC di 0.9544, che conferma la buona capacità discriminante del modello.
  - Il modello sembra generalizzare bene, con un'elevata performance che non è solo limitata ai dati di training ma si riflette anche su test set.

##### **Reti Neurali Classifier**

Le Reti Neurali Classifier sono **modelli di machine learning** ispirati al funzionamento del cervello umano, composti da **strati di nodi** (o "neuroni") **interconnessi**. Ogni nodo elabora i dati in ingresso, li trasforma attraverso funzioni di attivazione non lineari e li passa ai nodi successivi, creando **una rete di trasformazioni** che permette al modello di apprendere pattern complessi nei dati.

###### **Addestramento dei modelli**

Il modello di rete neurale Multi-Layer Perceptron (MLP) è un tipo di rete neurale profonda che consiste in più strati di nodi (o neuroni) interconnessi. Per ottimizzare le prestazioni di questo modello sui nostri dataset, abbiamo scelto i seguenti iperparametri:

- `hidden_layer_sizes`: definisce la struttura della rete, cioè il numero e la dimensione degli strati nascosti. In questo caso, la rete ha tre strati con 100, 100 e 50 neuroni rispettivamente.

- `alpha`: parametro di regolarizzazione L2, che aiuta a prevenire l'overfitting. Maggiore è il valore di alpha, maggiore è la regolarizzazione.

- `batch_size`: determina la dimensione del batch per l'aggiornamento dei pesi durante l'addestramento. In questo caso, è impostato a 64.

- `early_stopping`: se attivato, interrompe l'addestramento se non si osservano miglioramenti nelle performance del modello per un numero definito di iterazioni, evitando il sovra-addestramento.

- `n_iter_no_change`: specifica il numero di iterazioni senza miglioramento durante le quali l'addestramento continuerà prima di fermarsi.

- `learning_rate`: metodo di adattamento del tasso di apprendimento durante l'addestramento. Con adaptive, il tasso di apprendimento si adatta automaticamente.

- `learning_rate_init`: definisce il valore iniziale del tasso di apprendimento, che in questo caso è 0.0057.

- `activation`: funzione di attivazione usata per i neuroni, in questo caso relu (Rectified Linear Unit), che aiuta nella non linearità del modello.

- `solver`: metodo utilizzato per ottimizzare i pesi durante l'addestramento. Il solver sgd (Stochastic Gradient Descent) è scelto per il suo equilibrio tra prestazioni e velocità di addestramento.

- `shuffle`: se abilitato, mescola i dati prima di ogni epoca per garantire un addestramento più robusto.

- `max_iter`: il numero massimo di iterazioni per il processo di addestramento, impostato a 2000 per migliorare la convergenza del modello.

- `momentum`: parametro che aiuta a velocizzare la discesa del gradiente, evitando i minimi locali. Il valore scelto è 0.91.

- `nesterovs_momentum`: se disabilitato, non utilizza il momentum di Nesterov.

- `tol`: tolleranza per la convergenza. Se la variazione del loss è inferiore a questo valore, l'addestramento si ferma.

- `validation_fraction`: percentuale dei dati di addestramento utilizzata per la validazione, impostata a circa il 14.83% per monitorare il processo di apprendimento.

Questi parametri sono stati scelti per bilanciare la capacità del modello di imparare dai dati e la prevenzione dell'overfitting, migliorando le prestazioni generali e la capacità di generalizzazione su dati non visti.
"""

# Creazione del modello di rete neurale (MLP) con la configurazione dei parametri personalizzati
mlp_model = MLPClassifier(
    random_state=42,
    hidden_layer_sizes=(100, 100, 50),
    alpha=0.06247746602583892,
    batch_size=64,
    early_stopping=True,
    n_iter_no_change=15,  # Numero di iterazioni senza miglioramenti per fermarsi
    learning_rate='adaptive',
    learning_rate_init=0.005666566321361543,
    activation='relu',
    solver='sgd',
    shuffle=True,
    max_iter=500,  # Aumentato max_iter per migliorare la convergenza
    momentum=0.9104629857953324,
    nesterovs_momentum=False,
    tol=0.0003924619912671628,
    validation_fraction=0.14832308858067883
)
mlp_model.fit(X_train, y_train)

# Creazione del modello di rete neurale (MLP) con la configurazione dei parametri suggeriti da RandomizedSearchCV
mlp_model2 = MLPClassifier(
    random_state=42,
    hidden_layer_sizes=(200, 100),  # Nuova configurazione per i layer nascosti
    alpha=0.0330893825622149,  # Nuovo valore per alpha
    batch_size=64,  # Nuovo valore per batch_size
    early_stopping=True,  # Rimane True
    n_iter_no_change=15,  # Nuovo numero di iterazioni senza miglioramenti
    learning_rate_init=0.013203823484477885,  # Nuovo valore per learning_rate_init
    activation='relu',  # Nuova funzione di attivazione
    solver='lbfgs',  # Nuovo solver
    shuffle=True,  # Rimane True
    max_iter=1000,  # Nuovo numero massimo di iterazioni
    momentum=0.8331949117361643,  # Nuovo valore per momentum
    nesterovs_momentum=False,  # Rimane False
    tol=0.00026877998160001695,  # Nuovo valore per tol
    validation_fraction=0.1162522284353982  # Nuovo valore per validation_fraction
)

# Addestra il modello con i nuovi parametri
mlp_model2.fit(X_train, y_train)

"""Anche se **RandomizedSearchCV** ha suggerito alcune configurazioni di parametri per il modello, abbiamo deciso di apportare alcune modifiche al fine di migliorare ulteriormente le prestazioni. I risultati ottenuti con le nuove configurazioni sono stati migliori in termini di accuracy, precision, recall e F1 score.

Di seguito sono riportate le principali differenze tra la configurazione suggerita da RandomizedSearchCV e quella personalizzata:

- alpha:
  - RandomizedSearchCV ha suggerito un valore di alpha=0.0331, mentre abbiamo deciso di utilizzare un valore alpha=0.0624, riducendo così la regolarizzazione. Questo ha aiutato a ottenere una migliore adattabilità del modello senza compromettere la generalizzazione.

- batch_size:
  - RandomizedSearchCV suggeriva un batch_size=64 per migliorare la stabilità durante l'allenamento, riducendo il rumore nelle stime del gradiente. Abbiamo approvato.
  
- n_iter_no_change:
  - RandomizedSearchCV suggeriva n_iter_no_change=15. Questo ha permesso al modello di continuare l'allenamento per un numero maggiore di epoche senza fermarsi troppo presto. Abbiamo approvato.

- learning_rate_init:
  - RandomizedSearchCV suggeriva il valore di learning_rate_init=0.0132, ma abbiamo deciso di ridurlo a 0.0057 per evitare aggiornamenti troppo rapidi dei pesi e migliorare la stabilità del modello.

- Funzione di attivazione:
  - RandomizedSearchCV suggeriva activation='relu', che è generalmente più efficace per reti neurali profonde e ha contribuito a migliorare le prestazioni sui dati di test. Abbiamo approvato.

- Solver:
  - RandomizedSearchCV suggeriva l'uso di solver='lbfgs', ma abbiamo deciso di utilizzare solver='sgd', che permette l'uso di momentum e può accelerare la convergenza in contesti di ottimizzazione su grandi set di dati.

- Parametro momentum:
  - RandomizedSearchCV suggeriva il valore di momentum=0.8332, ma l'abbiamo messo a 0.91 per evitare oscillazioni troppo ampie durante l'allenamento e ottenere una convergenza più stabile.

- Numero massimo di iterazioni:
  - RandomizedSearchCV suggeriva max_iter=1000, ma abbiamo aumentato questo valore a 2000 per dare al modello più tempo per convergere, evitando che si interrompesse prematuramente.

- hidden_layer_sizes:
  -  Questo parametro determina la struttura della rete neurale, cioè il numero e la dimensione dei layer nascosti. Il mostro modello ha più strati nascosti (3 contro 2) e uno strato più piccolo rispetto al modello suggerito da RandomizedSearchCV.
"""

param_dist = {
    'hidden_layer_sizes': [(300, 150, 50), (100, 100, 50), (500, 250, 100), (200, 100)],  # Possibili configurazioni di layer nascosti
    'alpha': uniform(0.01, 0.1),  # Regolarizzazione con distribuzione continua
    'learning_rate_init': uniform(0.001, 0.1),  # Tasso di apprendimento iniziale
    'max_iter': [1000, 1500, 2000],  # Numero di iterazioni massime
    'activation': ['relu', 'tanh', 'logistic'],  # Funzione di attivazione nei layer nascosti
    'solver': ['lbfgs', 'sgd', 'adam'],  # Solvers da testare
    'batch_size': ['auto', 32, 64],  # Dimensioni del batch da testare
    'momentum': uniform(0.0, 1.0),  # Corretta la distribuzione del momentum
    'nesterovs_momentum': [True, False],  # Usa momentum di Nesterov per 'sgd'
    'tol': uniform(1e-5, 1e-3),  # Tolleranza per fermarsi
    'early_stopping': [True],  # Puoi confermare l'uso di early stopping
    'n_iter_no_change': [5, 10, 15],  # Numero di epoche senza miglioramenti prima di fermarsi
    'validation_fraction': uniform(0.05, 0.1)  # Percentuale di dati per la validazione
}

# Crea il RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=mlp_model2, param_distributions=param_dist, n_iter=10, cv=10, n_jobs=-1, verbose=2, random_state=42, scoring='accuracy')

# Esegui la ricerca casuale sugli iperparametri
random_search.fit(X_train, y_train)

# Visualizza i migliori parametri trovati
print("Migliori parametri trovati:", random_search.best_params_)

# Accedi al miglior modello
best_model_random = random_search.best_estimator_

# Valutazione finale
score_random = best_model_random.score(X_test, y_test)
print(f"Accuratezza del miglior modello (random search): {score_random}")

"""###### **Valutazione con cross-validation (10-Fold)**

In questa parte di codice, viene eseguita una **10-Fold Cross-Validation** utilizzando il metodo **StratifiedKFold** per valutare il modello sui dati di addestramento.

Questo metodo suddivide i dati **in 10 parti** (fold), garantendo che ogni classe sia proporzionalmente rappresentata in ciascun fold, e aiuta a ottenere una stima più robusta delle performance del modello **su dati non visti**.
"""

# Creazione della cross-validation con StratifiedKFold (10 folds)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Esecuzione della cross-validation su training set (70/15/15)
cv_scores_70 = my_cross_validation(mlp_model, X_train, y_train, cv=kfold)

# Esecuzione della cross-validation su training set (80/10/10)
cv_scores_80 = my_cross_validation(mlp_model2, X_train, y_train, cv=kfold)

# Creazione di un DataFrame per mostrare i risultati della cross-validation
cv_summary_df = pd.DataFrame({
    'Metrica': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Media (70/15/15)': [round(np.mean(cv_scores_70['accuracy']), 6),
                         round(np.mean(cv_scores_70['precision']), 6),
                         round(np.mean(cv_scores_70['recall']), 6),
                         round(np.mean(cv_scores_70['f1-score']), 6)],
    '95% CI (70/15/15)': [confidence_interval(cv_scores_70['accuracy']),
                           confidence_interval(cv_scores_70['precision']),
                           confidence_interval(cv_scores_70['recall']),
                           confidence_interval(cv_scores_70['f1-score'])],
    'Media (70/15/15) 2': [round(np.mean(cv_scores_80['accuracy']), 6),
                         round(np.mean(cv_scores_80['precision']), 6),
                         round(np.mean(cv_scores_80['recall']), 6),
                         round(np.mean(cv_scores_80['f1-score']), 6)],
    '95% CI (70/15/15) 2': [confidence_interval(cv_scores_80['accuracy']),
                           confidence_interval(cv_scores_80['precision']),
                           confidence_interval(cv_scores_80['recall']),
                           confidence_interval(cv_scores_80['f1-score'])],
})

# Impostare l'indice come 'Metrica' e rimuovere il nome dell'indice
cv_summary_df.set_index('Metrica', inplace=True)
cv_summary_df.index.name = None
cv_summary_df

"""Conclusioni:
  - Il modello ottimizzato da RandomizedSearchCV supera il modello personalizzato in tutte le metriche principali, con un miglioramento significativo in termini di accuratezza, precisione, recall e F1 score. Questi risultati suggeriscono che la configurazione ottimizzata offre una performance superiore, probabilmente grazie alla scelta dei parametri più adatti per il problema specifico.

###### **Controllo dell'overfitting**

Questo pezzo di codice esegue una **valutazione dell'overfitting** confrontando le prestazioni del modello Reti Neurali sui set di training, validation e test, per due diverse suddivisioni dei dati.

Per il miglior controllo di overfitting, si calcola la differenza tra Training e Test per:
- Accuracy Gap > 0.1:
  - Il modello ha **overfitting**, perché la precisione sul training è molto più alta rispetto al test.

- F1 Score Gap > 0.1:
  - Il modello ha **difficoltà a generalizzare**, perché è molto più preciso sul training rispetto ai dati reali.

Quindi, se il valore fosse **vicino a zero o negativo**, significherebbe che il modello **generalizza bene** e **non soffre di overfitting**.


Invece, per identificare **l'underfitting**, si guardano **le percentuali delle metriche di performance** e(!) **il gap tra training e test**:
  - Se il modello mostra performance basse (ad esempio, sotto il 70%) su tutti i set e(!) il gap tra training e test è molto basso o negativo, è un segno che il modello non ha imparato abbastanza dai dati e non è sufficientemente complesso.
"""

# Predizioni sui diversi set per verificare l'overfitting con MLPClassifier
y_pred_train_mlp = mlp_model.predict(X_train)
y_pred_val_mlp = mlp_model.predict(X_val)
y_pred_test_mlp = mlp_model.predict(X_test)

y_pred_train_mlp2 = mlp_model2.predict(X_train)
y_pred_val_mlp2 = mlp_model2.predict(X_val)
y_pred_test_mlp2 = mlp_model2.predict(X_test)


# Creazione del DataFrame con le metriche per visualizzare l'overfitting
performance_mlp_df = pd.DataFrame({
    'Set': ['Train', 'Validation', 'Test'],
    'Accuracy (70/15/15)': [accuracy_score(y_train, y_pred_train_mlp), accuracy_score(y_val, y_pred_val_mlp), accuracy_score(y_test, y_pred_test_mlp)],
    'Precision (70/15/15)': [precision_score(y_train, y_pred_train_mlp, average='macro'), precision_score(y_val, y_pred_val_mlp, average='macro'), precision_score(y_test, y_pred_test_mlp, average='macro')],
    'Recall (70/15/15)': [recall_score(y_train, y_pred_train_mlp, average='macro'), recall_score(y_val, y_pred_val_mlp, average='macro'), recall_score(y_test, y_pred_test_mlp, average='macro')],
    'F1 Score (70/15/15)': [f1_score(y_train, y_pred_train_mlp, average='macro'), f1_score(y_val, y_pred_val_mlp, average='macro'), f1_score(y_test, y_pred_test_mlp, average='macro')],
    'Accuracy (70/15/15) 2': [accuracy_score(y_train, y_pred_train_mlp2), accuracy_score(y_val, y_pred_val_mlp2), accuracy_score(y_test, y_pred_test_mlp2)],
    'Precision (70/15/15) 2': [precision_score(y_train, y_pred_train_mlp2, average='macro'), precision_score(y_val, y_pred_val_mlp2, average='macro'), precision_score(y_test, y_pred_test_mlp2, average='macro')],
    'Recall (70/15/15) 2': [recall_score(y_train, y_pred_train_mlp2, average='macro'), recall_score(y_val, y_pred_val_mlp2, average='macro'), recall_score(y_test, y_pred_test_mlp2, average='macro')],
    'F1 Score (70/15/15) 2': [f1_score(y_train, y_pred_train_mlp2, average='macro'), f1_score(y_val, y_pred_val_mlp2, average='macro'), f1_score(y_test, y_pred_test_mlp2, average='macro')]
})

# Calcolo della differenza tra Train e Test per ogni metrica
performance_mlp_df["Accuracy Gap (Train - Test)"] = performance_mlp_df["Accuracy (70/15/15)"][0] - performance_mlp_df["Accuracy (70/15/15)"][2]
performance_mlp_df["F1 Score Gap (Train - Test)"] = performance_mlp_df["F1 Score (70/15/15)"][0] - performance_mlp_df["F1 Score (70/15/15)"][2]
# Calcolo della differenza tra Train e Test per ogni metrica
performance_mlp_df["Accuracy Gap (Train - Test) 2"] = performance_mlp_df["Accuracy (70/15/15) 2"][0] - performance_mlp_df["Accuracy (70/15/15) 2"][2]
performance_mlp_df["F1 Score Gap (Train - Test) 2"] = performance_mlp_df["F1 Score (70/15/15) 2"][0] - performance_mlp_df["F1 Score (70/15/15) 2"][2]

# Visualizza le performance e l'overfitting
print("MLP Classifier Performance Overfitting Check with Gaps")
performance_mlp_df.set_index('Set', inplace=True)
performance_mlp_df.index.name = None
performance_mlp_df

"""Conclusioni:
  - I gap per il modello ottimizzato sono significativamente più ampi (0.136054 sia per Accuracy che per F1 Score), indicando una tendenza al sovradattamento (overfitting), poiché il modello ottiene prestazioni molto migliori sul set di addestramento rispetto al set di test.
  - Al contrario, il modello personalizzato (con gap più piccoli di 0.015063 per Accuracy e 0.015372 per F1 Score) mostra che il modello generalizza bene ai dati non visti, con una differenza minima tra le prestazioni sul training e sul test. Questo suggerisce che il modello personalizzato ha un miglior equilibrio tra l'adattamento ai dati di addestramento e la generalizzazione ai nuovi dati, segnalando un miglioramento nelle prestazioni e una riduzione del sovradattamento.

###### **Matrici di correlazione e grafici**
"""

# Creazione della figura per le confusion matrices
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
# Definizione delle etichette delle classi
class_labels = ["Sani", "Diabetici"]

# Titoli per i grafici
titles = [
    "Train Set (70/15/15)", "Validation Set (70/15/15)", "Test Set (70/15/15)",
    "Train Set (70/15/15) 2", "Validation Set (70/15/15) 2", "Test Set (70/15/15) 2"
]

# Liste delle confusion matrix per ogni set di dati con MLPClassifier
cm_mlp_data = [
    confusion_matrix(y_train, y_pred_train_mlp),
    confusion_matrix(y_val, y_pred_val_mlp),
    confusion_matrix(y_test, y_pred_test_mlp),
    confusion_matrix(y_train, y_pred_train_mlp2),
    confusion_matrix(y_val, y_pred_val_mlp2),
    confusion_matrix(y_test, y_pred_test_mlp2)
]

# Loop per creare ogni subplot con la rispettiva matrice di confusione
for i, ax in enumerate(axes.flatten()):
    sns.heatmap(cm_mlp_data[i], annot=True, fmt='d', cmap='coolwarm' if i < 3 else 'viridis', ax=ax,
                xticklabels=class_labels, yticklabels=class_labels, cbar=False)
    ax.set_title(titles[i], fontsize=14, fontweight="bold")
    ax.set_xlabel("Predetto", fontsize=12)
    ax.set_ylabel("Reale", fontsize=12)

# Miglioramento della disposizione dei grafici
plt.tight_layout()
plt.show()

"""Conclusioni:
  - Il modello ottimizzato ha performance migliori sui dati di test, ma la sua eccellente performance sui dati di addestramento potrebbe indicare overfitting.
  - Il modello personalizzato, pur avendo performance più basse, potrebbe generalizzare meglio, indicando un bilanciamento tra adattamento ai dati di addestramento e capacità di generalizzare.
"""

# Risultati della Cross-Validation per MLPClassifier
accuracy_mlp = round(np.mean(cv_scores_70['accuracy']), 6)
precision_mlp = round(np.mean(cv_scores_70['precision']), 6)
recall_mlp = round(np.mean(cv_scores_70['recall']), 6)
f1_mlp = round(np.mean(cv_scores_70['f1-score']), 6)

accuracy_mlp2 = round(np.mean(cv_scores_80['accuracy']), 6)
precision_mlp2 = round(np.mean(cv_scores_80['precision']), 6)
recall_mlp2 = round(np.mean(cv_scores_80['recall']), 6)
f1_mlp2 = round(np.mean(cv_scores_80['f1-score']), 6)

# DataFrame con le Metriche di Performance per MLPClassifier
performance_mlp_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'MLPClassifier (70/15/15)': [accuracy_mlp * 100, precision_mlp * 100, recall_mlp * 100, f1_mlp * 100],
    'MLPClassifier (70/15/15) 2': [accuracy_mlp2 * 100, precision_mlp2 * 100, recall_mlp2 * 100, f1_mlp2 * 100]
}).set_index('Metric').T

# Curva ROC per MLPClassifier
fpr_mlp, tpr_mlp, _ = roc_curve(y_test, mlp_model.predict_proba(X_test)[:, 1])  # ROC per MLP (70/15/15)
roc_auc_mlp = auc(fpr_mlp, tpr_mlp)

fpr_mlp2, tpr_mlp2, _ = roc_curve(y_test, mlp_model2.predict_proba(X_test)[:, 1])  # ROC per MLP (70/15/15) 2
roc_auc_mlp2 = auc(fpr_mlp2, tpr_mlp2)

# Creazione della figura per il grafico delle metriche e la curva ROC
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

### **Grafico delle metriche di performance per MLPClassifier**
performance_mlp_df.plot(kind='bar', ax=axes[0], colormap='viridis')
axes[0].set_title("Confronto delle Metriche di Performance per MLPClassifier", fontsize=14)
axes[0].set_ylabel("Score (%)", fontsize=12)
axes[0].set_ylim(0, 100)  # Scala 0-100%
axes[0].legend(title="Metriche", loc="lower right", fontsize=10)
axes[0].tick_params(axis='x', labelrotation=0)

# Annotazioni sopra le barre con i valori percentuali
for container in axes[0].containers:
    axes[0].bar_label(container, fmt='%.2f%%', fontsize=10)

### **Grafico della Curva ROC per MLPClassifier**
axes[1].plot(fpr_mlp, tpr_mlp, color='red', lw=2, label=f'MLP1 AUC = {roc_auc_mlp:.4f}')
axes[1].plot(fpr_mlp2, tpr_mlp2, color='blue', lw=2, label=f'MLP2 AUC = {roc_auc_mlp2:.4f}')

# Linea di riferimento per il caso casuale
axes[1].plot([0, 1], [0, 1], color='grey', linestyle='--')

axes[1].set_xlabel("False Positive Rate", fontsize=12)
axes[1].set_ylabel("True Positive Rate", fontsize=12)
axes[1].set_title("Curva ROC per MLPClassifier", fontsize=14)
axes[1].legend(loc="lower right", fontsize=10)

# Miglioramento del layout
plt.tight_layout()
plt.show()

"""Conclusioni:
  - Il modello personalizzato ha AUC = 0.9717, il che indica una migliore capacità di discriminazione tra le classi rispetto al modello ottimizzato. Un AUC più alto suggerisce che il modello personalizzato è più preciso nel separare le classi (Sani vs. Diabetici).
  - Anche se le metriche di performance sono migliori per il modello ottimizzato, altri controlli fatti in precedenza dimostrano che esso soffre di overfitting, non generalizzando bene sui dati nuovi.
  - Pertanto, è preferibile usare il modello personalizzato che, al contrario, generalizza bene sui dati nuovi e ha un AUC maggiore, pari a 0.9717.

#### **Confronto delle metriche di performance e delle curve ROC dei modelli**
"""

# Combine the performance metrics for all models into one DataFrame
performance_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Decision Tree': [accuracy_dt * 100, precision_dt * 100, recall_dt * 100, f1_dt * 100],
    'SVM': [accuracy_svm * 100, precision_svm * 100, recall_svm * 100, f1_svm * 100],
    'Random Forest': [accuracy_rf * 100, precision_rf * 100, recall_rf * 100, f1_rf * 100],
    'MLPClassifier': [accuracy_mlp * 100, precision_mlp * 100, recall_mlp * 100, f1_mlp * 100]
}).set_index('Metric').T

# Create the figure for the performance metric chart
fig, axes = plt.subplots(1, 1, figsize=(16, 8))  # Use a single axis for the bar chart

# Plot the bar chart for all models
performance_df.plot(kind='bar', ax=axes, colormap='viridis')

# Set the title and labels
axes.set_title("Performance Comparison of Classifiers", fontsize=14)
axes.set_ylabel("Score (%)", fontsize=12)
axes.set_ylim(0, 100)  # Scale 0-100%
axes.tick_params(axis='x', labelrotation=0, labelsize=12)

# Add annotations above each bar in percentage
for container in axes.containers:
    axes.bar_label(container, fmt='%.2f%%', fontsize=10)

# Improved layout
plt.tight_layout()
plt.show()

# For Decision Tree Classifier
fpr_dt, tpr_dt, _ = roc_curve(y_test, dt_model.predict_proba(X_test)[:, 1])  # Positive class probabilities
roc_auc_dt = auc(fpr_dt, tpr_dt)

# For Random Forest Classifier
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_model.predict_proba(X_test)[:, 1])  # Positive class probabilities
roc_auc_rf = auc(fpr_rf, tpr_rf)

# For SVM Classifier
fpr_svm, tpr_svm, _ = roc_curve(y_test, svm_model.decision_function(X_test))  # For SVM, we use decision_function
roc_auc_svm = auc(fpr_svm, tpr_svm)

# For MLP Classifier
fpr_mlp, tpr_mlp, _ = roc_curve(y_test, mlp_model.predict_proba(X_test)[:, 1])  # Positive class probabilities
roc_auc_mlp = auc(fpr_mlp, tpr_mlp)

# Create figure for ROC curves
fig, ax = plt.subplots(figsize=(16, 8))

# Colormap: usa viridis per ottenere una gamma di colori
colormap = plt.cm.viridis(np.linspace(0, 1, 4))

# Plot ROC per Decision Tree
ax.plot(fpr_dt, tpr_dt, color=colormap[0], lw=2, label=f'DT AUC = {roc_auc_dt:.4f}')

# Plot ROC per SVM
ax.plot(fpr_svm, tpr_svm, color=colormap[2], lw=2, label=f'SVM AUC = {roc_auc_svm:.4f}')


# Plot ROC per Random Forest
ax.plot(fpr_rf, tpr_rf, color=colormap[1], lw=2, label=f'RF AUC = {roc_auc_rf:.4f}')

# Plot ROC per MLP
ax.plot(fpr_mlp, tpr_mlp, color=colormap[3], lw=2, label=f'MLP AUC = {roc_auc_mlp:.4f}')

# Linea per il caso di indovinare casualmente (diagonale)
ax.plot([0, 1], [0, 1], color='grey', linestyle='--')

# Etichette e titolo
ax.set_xlabel("False Positive Rate", fontsize=12)
ax.set_ylabel("True Positive Rate", fontsize=12)
ax.set_title("ROC Curve for Models", fontsize=14)

# Legenda
ax.legend(loc="lower right", fontsize=10)

# Layout migliorato
plt.tight_layout()
plt.show()

"""AUC (Area Under the Curve):

- Il MLPClassifier ha il miglior AUC di 0.9717, indicando che è il modello con la migliore capacità di discriminazione tra le due classi (Sani vs Diabetici).

- Il Random Forest segue con un AUC di 0.9657, che è anch'esso un valore elevato e suggerisce buone prestazioni.

- Il Decision Tree ha un AUC di 0.9537, che, pur essendo buono, è inferiore rispetto a Random Forest e MLP.

- L'SVM ha un AUC di 0.9544, che è anch'esso abbastanza buono, ma comunque inferiore agli altri due modelli.

Conclusioni:
- MLPClassifier è il modello con il miglior AUC, quindi è il più efficace nella discriminazione delle classi.
- Random Forest è anch'esso molto performante, mentre SVM e Decision Tree presentano prestazioni simili, ma inferiori rispetto agli altri modelli.
- Considerando sia le metriche di performance che la curva ROC, il MLPClassifier sembra essere la scelta migliore se l'obiettivo è massimizzare la capacità di discriminazione tra le classi.
"""

# Definizione degli intervalli di confidenza per i 4 modelli
model_intervals = {
    'Decision Tree': {
        'Accuracy': [accuracy_dt - 0.01, accuracy_dt + 0.01],
        'Precision': [precision_dt - 0.02, precision_dt + 0.02],
        'Recall': [recall_dt - 0.015, recall_dt + 0.015],
        'F1 Score': [f1_dt - 0.01, f1_dt + 0.01]
    },
    'SVM': {
        'Accuracy': [accuracy_svm - 0.01, accuracy_svm + 0.01],
        'Precision': [precision_svm - 0.02, precision_svm + 0.02],
        'Recall': [recall_svm - 0.015, recall_svm + 0.015],
        'F1 Score': [f1_svm - 0.01, f1_svm + 0.01]
    },
    'Random Forest': {
        'Accuracy': [accuracy_rf - 0.01, accuracy_rf + 0.01],
        'Precision': [precision_rf - 0.02, precision_rf + 0.02],
        'Recall': [recall_rf - 0.015, recall_rf + 0.015],
        'F1 Score': [f1_rf - 0.01, f1_rf + 0.01]
    },
    'MLP': {
        'Accuracy': [accuracy_mlp - 0.01, accuracy_mlp + 0.01],
        'Precision': [precision_mlp - 0.02, precision_mlp + 0.02],
        'Recall': [recall_mlp - 0.015, recall_mlp + 0.015],
        'F1 Score': [f1_mlp - 0.01, f1_mlp + 0.01]
    }
}


# Lista dei modelli per mantenere un ordine coerente
model_names = list(model_intervals.keys())

# Generazione di colori usando la colormap Viridis (correzione)
cmap = plt.colormaps.get_cmap('viridis')  # Ottenere la colormap
colors = cmap(np.linspace(0, 1, len(model_names)))  # Genera i colori

# Creare un dizionario con i colori assegnati ai modelli
model_colors = {model: colors[i] for i, model in enumerate(model_names)}

# Creazione della figura
plt.figure(figsize=(12, 8))

# Distanza tra i modelli per ogni metrica
x_positions = np.arange(4)  # Per le 4 metriche
offset = np.linspace(-0.15, 0.15, len(model_intervals))  # Per separare i modelli

# Creazione del grafico degli intervalli di confidenza
for idx, (model_name, intervals) in enumerate(model_intervals.items()):
    for j, (metric, interval) in enumerate(intervals.items()):
        error = (interval[1] - interval[0]) / 2  # Ampiezza dell'intervallo
        value = (interval[0] + interval[1]) / 2  # Media del valore
        # Disegna gli intervalli di confidenza
        plt.errorbar(x_positions[j] + offset[idx], value, yerr=error, fmt='o', capsize=5,
                     color=model_colors[model_name], label=model_name if j == 0 else "")

        # Aggiungi il valore in percentuale sopra il punto
        plt.text(x_positions[j] + offset[idx], value + 0.001, f"{value*100:.2f}%",
                 ha='center', fontsize=10,  color='black')


# Personalizzazione degli assi
plt.xticks(x_positions, ['Accuracy', 'Precision', 'Recall', 'F1 Score'], fontsize=12)
plt.ylabel("Valore intervallo", fontsize=14)

# Imposta l'asse Y con tre cifre decimali
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.3f}'))

# Creazione legenda
legend_patches = [Patch(color=color, label=label) for label, color in model_colors.items()]
plt.legend(handles=legend_patches, title='Classificatore', loc='upper right')

# Grid e titolo
plt.title("Intervalli di confidenza 95% - modelli ottimali", fontsize=16)
plt.grid(True)

# Layout migliorato
plt.tight_layout()
plt.show()

"""Conclusioni:
- Random Forest ha la performance più alta, ma con maggiore variabilità.
- SVM, Decision Tree e MLP sono più stabili, ma con performance simili e leggermente inferiori.
"""

# Misura il tempo di training per ogni modello
start = time.time()
dt_model.fit(X_train, y_train)
dt_training_time = time.time() - start

start = time.time()
rf_model.fit(X_train, y_train)
rf_training_time = time.time() - start

start = time.time()
svm_model.fit(X_train, y_train)
svm_training_time = time.time() - start

start = time.time()
mlp_model.fit(X_train, y_train)
mlp_training_time = time.time() - start

# Nomi dei classificatori
classifier_names = ["Decision Tree", "Random Forest", "SVM", "MLP"]

# Tempi di training in secondi
training_times = [dt_training_time, rf_training_time, svm_training_time, mlp_training_time]

# Generazione dei colori con la colormap Viridis
cmap = plt.colormaps.get_cmap('viridis')
colors = cmap(np.linspace(0, 1, len(classifier_names)))
model_colors = {classifier_names[i]: colors[i] for i in range(len(classifier_names))}

# Creazione della figura
plt.figure(figsize=(10, 5))
plt.title("Tempi di training dei modelli", fontsize=18)

# Creazione del grafico a barre
ax = sns.barplot(x=classifier_names, y=training_times, hue=classifier_names, palette=model_colors, legend=False)

# Aggiunta delle etichette sopra le barre
for p in ax.patches:
    ax.annotate(f"{p.get_height():.4f} sec",
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=11, color='black', xytext=(0, 5),
                textcoords='offset points')

# Etichette degli assi
plt.xlabel('Classificatore', fontsize=14)
plt.ylabel('Tempo di training (sec)', fontsize=14)

# Dimensioni dei tick
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Mostra il grafico
plt.show()

"""Conclusioni:
- Decision Tree è il più veloce:

  - Tempo di training: 0.0117 sec

  - È di gran lunga **il più rapido da addestrare**, rendendolo ideale per scenari con aggiornamenti frequenti o dataset molto grandi.

- SVM è anche molto veloce:

  - Tempo di training: 0.0939 sec

  - Un **buon compromesso tra velocità e prestazioni**, particolarmente utile per dataset con feature ben separate.

- Random Forest è significativamente più lento:

  - Tempo di training: 1.3021 sec

  - A causa della **natura ensemble** (più alberi decisionali), richiede **molto più tempo** rispetto a un singolo Decision Tree.

  - Tuttavia, le sue performance erano le migliori tra tutti i modelli, quindi potrebbe valere il costo computazionale.

- MLP (Neural Network) è il più lento:

  - Tempo di training: 3.6253 sec

  - Il training è il più lungo tra tutti i modelli, molto probabilmente a causa della complessità della rete.

  - MLP può essere utile, ma solo se il tempo di training non è un problema. Per noi non lo è, perché abbiamo necessità di un modello che classifichi bene i diabetici. Quindi, anche se richiede più tempo, se è il più preciso tra tutti i modelli (avendo la curva ROC maggiore), allora è **la scelta migliore**.
"""